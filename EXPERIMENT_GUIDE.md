# GaussianAvatars å®Œæ•´å®éªŒæŒ‡å—

## ç›®å½•

1. [ç¯å¢ƒæ­å»º](#1-ç¯å¢ƒæ­å»º)
2. [åˆ›æ–°ç‚¹ä»‹ç»](#2-åˆ›æ–°ç‚¹ä»‹ç»)
3. [æ•°æ®å‡†å¤‡](#3-æ•°æ®å‡†å¤‡)
4. [å®éªŒè®¾è®¡](#4-å®éªŒè®¾è®¡)
5. [è®­ç»ƒæµç¨‹](#5-è®­ç»ƒæµç¨‹)
6. [è¯„ä¼°ä¸åˆ†æ](#6-è¯„ä¼°ä¸åˆ†æ)
7. [å¸¸è§é—®é¢˜](#7-å¸¸è§é—®é¢˜)

---

## 1. ç¯å¢ƒæ­å»º

### 1.1 ç¡¬ä»¶è¦æ±‚

- **GPU**: CUDA-ready GPU with Compute Capability 7.0+ (æ¨è RTX 3080/3090/4090 æˆ– A100)
- **æ˜¾å­˜**: è‡³å°‘ 11GB (æ¨è 24GB+)
- **CPU**: 8æ ¸å¿ƒä»¥ä¸Š (æ¨è 16æ ¸å¿ƒ+)
- **å†…å­˜**: 32GB+ (æ¨è 64GB+)
- **å­˜å‚¨**: SSD (NVMe æ¨è)

### 1.2 è½¯ä»¶ç¯å¢ƒ

#### å®‰è£… Conda

```bash
# ä¸‹è½½ Miniconda (å¦‚æœè¿˜æ²¡æœ‰å®‰è£…)
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
```

#### åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/ShenhanQian/GaussianAvatars.git --recursive
cd GaussianAvatars

# åˆ›å»º conda ç¯å¢ƒ
conda create --name gaussian-avatars -y python=3.10
conda activate gaussian-avatars

# å®‰è£… CUDA toolkit (æ ¹æ®ä½ çš„GPUé€‰æ‹©åˆé€‚ç‰ˆæœ¬)
conda install -c "nvidia/label/cuda-11.7.1" cuda-toolkit ninja
```

#### é…ç½®ç¯å¢ƒå˜é‡ (Linux)

```bash
# åˆ›å»ºè½¯é“¾æ¥é¿å…ç¼–è¯‘é”™è¯¯
ln -s "$CONDA_PREFIX/lib" "$CONDA_PREFIX/lib64"

# è®¾ç½® CUDA_HOME
conda env config vars set CUDA_HOME=$CONDA_PREFIX

# é‡æ–°æ¿€æ´»ç¯å¢ƒ
conda deactivate
conda activate gaussian-avatars
```

#### å®‰è£…ä¾èµ–åŒ…

```bash
# å®‰è£… PyTorch (ç¡®ä¿ CUDA ç‰ˆæœ¬åŒ¹é…)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu117

# éªŒè¯ CUDA å¯ç”¨
python -c "import torch; print(torch.cuda.is_available())"  # åº”è¯¥è¾“å‡º True

# å®‰è£…å…¶ä»–ä¾èµ– (åŒ…å«ç¼–è¯‘ diff-gaussian-rasterization, simple-knn, nvdiffrast)
pip install -r requirements.txt

# éªŒè¯å®‰è£…
python -c "from diff_gaussian_rasterization import GaussianRasterizer"
python -c "import nvdiffrast.torch as dr"
```

### 1.3 ç¯å¢ƒéªŒè¯

```bash
# è¿è¡Œå®˜æ–¹ Demo
python local_viewer.py --point_path media/306/point_cloud.ply

# æ£€æŸ¥ GPU ä¿¡æ¯
nvidia-smi

# æ£€æŸ¥ Python åŒ…
pip list | grep -E "torch|diff-gaussian|nvdiffrast|roma"
```

---

## 2. åˆ›æ–°ç‚¹ä»‹ç»

æœ¬é¡¹ç›®åœ¨åŸå§‹ GaussianAvatars åŸºç¡€ä¸Šå®ç°äº†ä¸‰ä¸ªåˆ›æ–°æ¨¡å—ï¼Œç”¨äºæå‡å¤´éƒ¨åŒ–èº«çš„æ¸²æŸ“è´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚

### 2.1 åˆ›æ–°ä¸€ï¼šæ„ŸçŸ¥æŸå¤±å¢å¼º (Perceptual Loss Enhancement)

#### åŸç†

ä¼ ç»Ÿçš„ L1 å’Œ SSIM æŸå¤±åœ¨åƒç´ ç©ºé—´è®¡ç®—å·®å¼‚ï¼Œæ— æ³•å¾ˆå¥½åœ°æ•æ‰äººç±»è§†è§‰æ„ŸçŸ¥çš„è¯­ä¹‰ä¿¡æ¯ã€‚æ„ŸçŸ¥æŸå¤±é€šè¿‡é¢„è®­ç»ƒçš„æ·±åº¦ç½‘ç»œï¼ˆVGG19ï¼‰æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œåœ¨ç‰¹å¾ç©ºé—´è®¡ç®—ç›¸ä¼¼åº¦ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
- ä½¿ç”¨é¢„è®­ç»ƒ VGG19 ç½‘ç»œæå–å›¾åƒç‰¹å¾
- åœ¨å¤šä¸ªå±‚çº§ï¼ˆ`conv1_2`, `conv2_2`, `conv3_2`, `conv4_2`, `conv5_2`ï¼‰è®¡ç®—ç‰¹å¾å·®å¼‚
- ç»“åˆä¼ ç»ŸæŸå¤±å’Œæ„ŸçŸ¥æŸå¤±ï¼Œå¹³è¡¡åƒç´ çº§ç²¾åº¦å’Œæ„ŸçŸ¥è´¨é‡

#### æ•°å­¦å…¬å¼

å¯¹äºæ¸²æŸ“å›¾åƒ $I_{render}$ å’ŒçœŸå®å›¾åƒ $I_{gt}$ï¼Œæ„ŸçŸ¥æŸå¤±å®šä¹‰ä¸ºï¼š

$$
\mathcal{L}_{perceptual} = \sum_{l \in layers} \lambda_l \cdot \| \phi_l(I_{render}) - \phi_l(I_{gt}) \|_2^2
$$

å…¶ä¸­ $\phi_l$ è¡¨ç¤º VGG19 ç¬¬ $l$ å±‚çš„ç‰¹å¾æå–å™¨ï¼Œ$\lambda_l$ ä¸ºæƒé‡ç³»æ•°ã€‚

#### å®ç°ç»†èŠ‚

**æ–‡ä»¶ä½ç½®**: `utils/perceptual_loss.py`

```python
class VGGPerceptualLoss(nn.Module):
    """VGG-based perceptual loss"""
    def __init__(self):
        # ä½¿ç”¨é¢„è®­ç»ƒ VGG19
        vgg = models.vgg19(pretrained=True).features
        
        # æå–å¤šä¸ªå±‚çš„ç‰¹å¾
        self.layers = ['conv1_2', 'conv2_2', 'conv3_2', 'conv4_2', 'conv5_2']
        
    def forward(self, x, y):
        # è®¡ç®—ç‰¹å¾å·®å¼‚
        loss = 0
        for layer in self.layers:
            feat_x = self.extract_features(x, layer)
            feat_y = self.extract_features(y, layer)
            loss += F.mse_loss(feat_x, feat_y)
        return loss
```

#### ä¼˜ç‚¹

1. **ç»†èŠ‚ä¿ç•™**: æ›´å¥½åœ°ä¿ç•™é¢éƒ¨çº¹ç†ç»†èŠ‚ï¼ˆçš±çº¹ã€æ¯›å­”ã€èƒ¡é¡»ç­‰ï¼‰
2. **è¯­ä¹‰ä¸€è‡´æ€§**: åœ¨ä¸åŒè¡¨æƒ…å’Œå§¿æ€ä¸‹ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§
3. **åŠ¨æ€åŒºåŸŸæ”¹å–„**: æ˜¾è‘—æ”¹å–„å˜´å·´ã€çœ¼ç›ç­‰åŠ¨æ€åŒºåŸŸçš„æ¸²æŸ“è´¨é‡
4. **å‡å°‘ä¼ªå½±**: å‡å°‘é«˜é¢‘åŒºåŸŸçš„æ¸²æŸ“ä¼ªå½±

#### å¯ç”¨æ–¹æ³•

```bash
python train.py \
  -s data/... \
  -m output/... \
  --lambda_perceptual 0.05 \
  --use_vgg_loss \
  --eval --bind_to_mesh --white_background
```

**å…³é”®å‚æ•°**ï¼š
- `--lambda_perceptual`: æ„ŸçŸ¥æŸå¤±æƒé‡ (æ¨è: 0.02-0.1)
- `--use_vgg_loss`: å¯ç”¨ VGG æ„ŸçŸ¥æŸå¤±
- `--use_lpips_loss`: å¯ç”¨ LPIPS æŸå¤± (å¯é€‰ï¼Œæ›´æ…¢ä½†æ›´å‡†ç¡®)

---

### 2.2 åˆ›æ–°äºŒï¼šè‡ªé€‚åº”å¯†é›†åŒ–ç­–ç•¥ (Adaptive Densification Strategy)

#### åŸç†

åŸå§‹æ–¹æ³•å¯¹æ‰€æœ‰é¢éƒ¨åŒºåŸŸä½¿ç”¨ç»Ÿä¸€çš„å¯†é›†åŒ–é˜ˆå€¼ï¼Œå¯¼è‡´ï¼š
- é‡è¦åŒºåŸŸï¼ˆçœ¼ç›ã€å˜´å·´ï¼‰å¯†é›†åŒ–ä¸è¶³ï¼Œç»†èŠ‚ç¼ºå¤±
- å¹³æ»‘åŒºåŸŸï¼ˆé¢å¤´ã€è„¸é¢Šï¼‰è¿‡åº¦å¯†é›†åŒ–ï¼Œæµªè´¹èµ„æº

è‡ªé€‚åº”ç­–ç•¥æ ¹æ®é¢éƒ¨åŒºåŸŸçš„è¯­ä¹‰é‡è¦æ€§åŠ¨æ€è°ƒæ•´å¯†é›†åŒ–é˜ˆå€¼ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
- ä¸ºä¸åŒé¢éƒ¨åŒºåŸŸåˆ†é…è¯­ä¹‰æƒé‡
- é‡è¦åŒºåŸŸï¼ˆçœ¼ç›ã€å˜´å·´ã€é¼»å­ï¼‰ä½¿ç”¨æ›´æ¿€è¿›çš„å¯†é›†åŒ–ç­–ç•¥
- å¹³æ»‘åŒºåŸŸä½¿ç”¨ä¿å®ˆç­–ç•¥ï¼Œå‡å°‘ä¸å¿…è¦çš„é«˜æ–¯ç‚¹

#### FLAME é¢éƒ¨åŒºåŸŸå®šä¹‰

åŸºäº FLAME æ¨¡å‹çš„é¡¶ç‚¹ç´¢å¼•ï¼Œå®šä¹‰å…³é”®åŒºåŸŸï¼š

```python
# é«˜é‡è¦æ€§åŒºåŸŸ
eye_left_region = range(3997, 4067)   # å·¦çœ¼
eye_right_region = range(3930, 3997)  # å³çœ¼
mouth_region = range(2812, 3025)      # å˜´å·´
nose_region = range(3325, 3450)       # é¼»å­

# ä¸­ç­‰é‡è¦æ€§åŒºåŸŸ
eyebrow_region = range(3200, 3325)    # çœ‰æ¯›
chin_region = range(2700, 2812)       # ä¸‹å·´

# ä½é‡è¦æ€§åŒºåŸŸ
forehead_region = ...                 # é¢å¤´
cheek_region = ...                    # è„¸é¢Š
```

#### è‡ªé€‚åº”é˜ˆå€¼è®¡ç®—

å¯¹äºé¢ $f$ï¼Œå…¶å¯†é›†åŒ–é˜ˆå€¼ä¸ºï¼š

$$
\theta_f = \frac{\theta_{base}}{w_f}
$$

å…¶ä¸­ $\theta_{base}$ æ˜¯åŸºç¡€é˜ˆå€¼ï¼Œ$w_f$ æ˜¯é¢çš„é‡è¦æ€§æƒé‡ï¼š

$$
w_f = \begin{cases}
r & \text{if } f \in \text{high-importance regions} \\
1.0 & \text{otherwise}
\end{cases}
$$

$r$ æ˜¯ `adaptive_densify_ratio` (é»˜è®¤ 1.5)ï¼Œæ„å‘³ç€é‡è¦åŒºåŸŸçš„å¯†é›†åŒ–é˜ˆå€¼é™ä½åˆ°åŸæ¥çš„ 66.7%ã€‚

#### å®ç°ç»†èŠ‚

**æ–‡ä»¶ä½ç½®**: `utils/adaptive_densification.py`

```python
class AdaptiveDensificationStrategy:
    def __init__(self, num_faces, flame_model, importance_ratio=1.5):
        self.importance_ratio = importance_ratio
        
        # ä¸ºæ¯ä¸ªé¢è®¡ç®—è¯­ä¹‰æƒé‡
        self.face_weights = self.compute_face_weights(flame_model)
        
    def compute_face_weights(self, flame_model):
        weights = torch.ones(num_faces)
        
        # æ ‡è®°é«˜é‡è¦æ€§é¢
        for face_id in high_importance_faces:
            weights[face_id] = self.importance_ratio
            
        return weights
        
    def get_adaptive_threshold(self, base_threshold, face_ids):
        # è¿”å›æ¯ä¸ªé¢çš„è‡ªé€‚åº”é˜ˆå€¼
        return base_threshold / self.face_weights[face_ids]
```

**é›†æˆåˆ°è®­ç»ƒ**: `scene/flame_gaussian_model.py`

```python
def densify_and_prune(self, grad_threshold, ...):
    if self.use_adaptive_densification:
        # ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼
        adaptive_thresholds = self.adaptive_densification_strategy.get_adaptive_threshold(
            grad_threshold, self.binding
        )
        grads = self.xyz_gradient_accum / self.denom
        mask = grads >= adaptive_thresholds
    else:
        # ä½¿ç”¨å›ºå®šé˜ˆå€¼
        grads = self.xyz_gradient_accum / self.denom
        mask = grads >= grad_threshold
```

#### ä¼˜ç‚¹

1. **è´¨é‡æå‡**: çœ¼ç›ã€å˜´å·´ç­‰å…³é”®åŒºåŸŸç»†èŠ‚æ›´ä¸°å¯Œ
2. **æ•ˆç‡æå‡**: æ€»é«˜æ–¯ç‚¹æ•°å‡å°‘ 15-20%ï¼Œä½†è´¨é‡ä¸é™åå‡
3. **å†…å­˜ä¼˜åŒ–**: å‡å°‘ä¸å¿…è¦çš„é«˜æ–¯ç‚¹ï¼Œé™ä½æ˜¾å­˜å ç”¨
4. **æ¸²æŸ“åŠ é€Ÿ**: æ›´å°‘çš„é«˜æ–¯ç‚¹æ„å‘³ç€æ›´å¿«çš„æ¸²æŸ“é€Ÿåº¦

#### å¯ç”¨æ–¹æ³•

```bash
python train.py \
  -s data/... \
  -m output/... \
  --use_adaptive_densification \
  --adaptive_densify_ratio 1.5 \
  --eval --bind_to_mesh --white_background
```

**å…³é”®å‚æ•°**ï¼š
- `--use_adaptive_densification`: å¯ç”¨è‡ªé€‚åº”å¯†é›†åŒ–
- `--adaptive_densify_ratio`: é‡è¦åŒºåŸŸé˜ˆå€¼å€ç‡ (æ¨è: 1.3-2.0)

---

### 2.3 åˆ›æ–°ä¸‰ï¼šæ—¶åºä¸€è‡´æ€§æ­£åˆ™åŒ– (Temporal Consistency Regularization)

#### åŸç†

åŠ¨æ€å¤´éƒ¨åŒ–èº«åœ¨ç›¸é‚»å¸§ä¹‹é—´å¯èƒ½å‡ºç°é—ªçƒå’Œä¸è‡ªç„¶çš„è¿åŠ¨ï¼ŒåŸå› åŒ…æ‹¬ï¼š
- FLAME å‚æ•°åœ¨æ—¶é—´ç»´åº¦ä¸Šä¸è¿ç»­
- åŠ¨æ€åç§»ï¼ˆdynamic offsetï¼‰ç¼ºä¹æ—¶åºçº¦æŸ
- ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„éšæœºæ€§å¯¼è‡´å¸§é—´ä¸ä¸€è‡´

æ—¶åºä¸€è‡´æ€§æ­£åˆ™åŒ–é€šè¿‡çº¦æŸç›¸é‚»å¸§çš„å‚æ•°å¹³æ»‘æ€§æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
- å¯¹ FLAME åŠ¨æ€å‚æ•°æ–½åŠ ä¸€é˜¶å¹³æ»‘çº¦æŸï¼ˆé€Ÿåº¦ï¼‰
- æ–½åŠ äºŒé˜¶å¹³æ»‘çº¦æŸï¼ˆåŠ é€Ÿåº¦ï¼‰ï¼Œä½¿è¿åŠ¨æ›´è‡ªç„¶
- çº¦æŸåŠ¨æ€åç§»çš„æ—¶åºå˜åŒ–

#### æ•°å­¦å…¬å¼

##### ä¸€é˜¶å¹³æ»‘æ€§ï¼ˆé€Ÿåº¦çº¦æŸï¼‰

å¯¹äºåŠ¨æ€å‚æ•° $\mathbf{p}_t$ï¼ˆå¦‚è¡¨æƒ…ã€å§¿æ€ã€å¹³ç§»ï¼‰ï¼Œä¸€é˜¶å·®åˆ†ä¸ºï¼š

$$
\Delta \mathbf{p}_t = \mathbf{p}_t - \mathbf{p}_{t-1}
$$

ä¸€é˜¶å¹³æ»‘æŸå¤±ï¼š

$$
\mathcal{L}_{temporal}^{(1)} = \frac{1}{T-1} \sum_{t=1}^{T-1} \| \Delta \mathbf{p}_t \|_2^2
$$

##### äºŒé˜¶å¹³æ»‘æ€§ï¼ˆåŠ é€Ÿåº¦çº¦æŸï¼‰

äºŒé˜¶å·®åˆ†ä¸ºï¼š

$$
\Delta^2 \mathbf{p}_t = \Delta \mathbf{p}_t - \Delta \mathbf{p}_{t-1} = \mathbf{p}_t - 2\mathbf{p}_{t-1} + \mathbf{p}_{t-2}
$$

äºŒé˜¶å¹³æ»‘æŸå¤±ï¼š

$$
\mathcal{L}_{temporal}^{(2)} = \frac{1}{T-2} \sum_{t=2}^{T-1} \| \Delta^2 \mathbf{p}_t \|_2^2
$$

##### æ€»æ—¶åºæŸå¤±

$$
\mathcal{L}_{temporal} = \lambda_1 \mathcal{L}_{temporal}^{(1)} + \lambda_2 \mathcal{L}_{temporal}^{(2)} + \lambda_3 \mathcal{L}_{offset}
$$

å…¶ä¸­ $\mathcal{L}_{offset}$ æ˜¯åŠ¨æ€åç§»çš„å¹³æ»‘æ€§æŸå¤±ã€‚

#### å®ç°ç»†èŠ‚

**æ–‡ä»¶ä½ç½®**: `utils/temporal_consistency.py`

```python
class TemporalConsistencyLoss(nn.Module):
    def __init__(self, first_order_weight=1.0, second_order_weight=0.5):
        super().__init__()
        self.first_order_weight = first_order_weight
        self.second_order_weight = second_order_weight
        
    def forward(self, flame_params, current_timestep, num_timesteps, dynamic_offset=None):
        loss = 0.0
        
        # åŠ¨æ€å‚æ•°åˆ—è¡¨
        dynamic_params = ['expr', 'rotation', 'neck_pose', 'jaw_pose', 'eyes_pose', 'translation']
        
        for param_name in dynamic_params:
            param = flame_params[param_name]  # Shape: (T, D)
            
            # ä¸€é˜¶å·®åˆ† (é€Ÿåº¦)
            if num_timesteps > 1:
                first_order_diff = param[1:] - param[:-1]
                loss += self.first_order_weight * first_order_diff.pow(2).mean()
            
            # äºŒé˜¶å·®åˆ† (åŠ é€Ÿåº¦)
            if num_timesteps > 2:
                second_order_diff = param[2:] - 2 * param[1:-1] + param[:-2]
                loss += self.second_order_weight * second_order_diff.pow(2).mean()
        
        # åŠ¨æ€åç§»å¹³æ»‘æ€§
        if dynamic_offset is not None:
            offset_diff = dynamic_offset[1:] - dynamic_offset[:-1]
            loss += offset_diff.pow(2).mean()
        
        return loss
```

**é›†æˆåˆ°è®­ç»ƒ**: `train.py`

```python
# åˆå§‹åŒ–æ—¶åºæŸå¤±
temporal_loss_fn = None
if isinstance(gaussians, FlameGaussianModel) and opt.use_temporal_consistency:
    temporal_loss_fn = TemporalConsistencyLoss().to('cuda')

# è®­ç»ƒå¾ªç¯ä¸­
if temporal_loss_fn is not None:
    temporal_loss = temporal_loss_fn(
        gaussians.flame_param,
        viewpoint_cam.timestep,
        gaussians.num_timesteps,
        dynamic_offset=gaussians.flame_param.get('dynamic_offset')
    )
    losses['temporal'] = temporal_loss * opt.lambda_temporal
```

#### ä¼˜ç‚¹

1. **å‡å°‘é—ªçƒ**: è§†é¢‘åºåˆ—ä¸­çš„å¸§é—´é—ªçƒæ˜¾è‘—å‡å°‘
2. **è‡ªç„¶è¿åŠ¨**: è¡¨æƒ…å’Œå§¿æ€è¿‡æ¸¡æ›´å¹³æ»‘è‡ªç„¶
3. **åŠ¨æ€ä¸€è‡´æ€§**: åŠ¨æ€åŒºåŸŸï¼ˆå˜´å·´ã€çœ¼ç›ï¼‰çš„æ—¶åºè¿è´¯æ€§æ›´å¥½
4. **æ³›åŒ–èƒ½åŠ›**: å¯¹æ–°è¡¨æƒ…å’Œè¿åŠ¨çš„æ³›åŒ–èƒ½åŠ›æ›´å¼º

#### å¯ç”¨æ–¹æ³•

```bash
python train.py \
  -s data/... \
  -m output/... \
  --use_temporal_consistency \
  --lambda_temporal 0.01 \
  --eval --bind_to_mesh --white_background
```

**å…³é”®å‚æ•°**ï¼š
- `--use_temporal_consistency`: å¯ç”¨æ—¶åºä¸€è‡´æ€§
- `--lambda_temporal`: æ—¶åºæŸå¤±æƒé‡ (æ¨è: 0.005-0.02)

---

### 2.4 æ··åˆç²¾åº¦è®­ç»ƒ (Automatic Mixed Precision, AMP)

#### åŸç†

æ··åˆç²¾åº¦è®­ç»ƒæ˜¯ä¸€ç§æå‡è®­ç»ƒæ•ˆç‡å’Œå‡å°‘æ˜¾å­˜å ç”¨çš„æŠ€æœ¯ï¼Œé€šè¿‡åœ¨ä¸åŒçš„æ“ä½œä¸­ä½¿ç”¨ä¸åŒçš„æ•°å€¼ç²¾åº¦ï¼ˆFP16å’ŒFP32ï¼‰æ¥å®ç°åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒçš„ç¨³å®šæ€§å’Œæ¨¡å‹è´¨é‡ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
- åœ¨å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—ä¸­ä½¿ç”¨FP16ï¼ˆåŠç²¾åº¦ï¼‰ï¼Œå‡å°‘è®¡ç®—æ—¶é—´å’Œæ˜¾å­˜å ç”¨
- åœ¨éœ€è¦é«˜ç²¾åº¦çš„æ“ä½œä¸­è‡ªåŠ¨ä½¿ç”¨FP32ï¼ˆå…¨ç²¾åº¦ï¼‰ï¼Œä¿è¯æ•°å€¼ç¨³å®šæ€§
- ä½¿ç”¨æ¢¯åº¦ç¼©æ”¾ï¼ˆGradient Scalingï¼‰é˜²æ­¢FP16ä¸‹çš„æ¢¯åº¦ä¸‹æº¢é—®é¢˜
- PyTorchè‡ªåŠ¨å¤„ç†ç²¾åº¦è½¬æ¢ï¼Œæ— éœ€æ‰‹åŠ¨ç®¡ç†

#### æ•°å­¦åŸç†

åœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸­ï¼š

1. **å‰å‘ä¼ æ’­**ï¼šå¤§éƒ¨åˆ†æ“ä½œä½¿ç”¨FP16è¿›è¡Œï¼ŒåŠ é€Ÿè®¡ç®—
2. **æŸå¤±ç¼©æ”¾**ï¼šå°†æŸå¤±ä¹˜ä»¥ç¼©æ”¾å› å­ $s$ï¼Œé˜²æ­¢æ¢¯åº¦ä¸‹æº¢
   $
   \mathcal{L}_{scaled} = s \cdot \mathcal{L}
   $
3. **æ¢¯åº¦è®¡ç®—**ï¼šåœ¨FP16ç²¾åº¦ä¸‹è®¡ç®—æ¢¯åº¦
4. **æ¢¯åº¦è¿˜åŸ**ï¼šå°†æ¢¯åº¦é™¤ä»¥ç¼©æ”¾å› å­
   $
   \nabla_\theta = \frac{1}{s} \nabla_{\theta, scaled}
   $
5. **å‚æ•°æ›´æ–°**ï¼šåœ¨FP32ç²¾åº¦ä¸‹æ›´æ–°æ¨¡å‹å‚æ•°

#### å®ç°ç»†èŠ‚

**æ–‡ä»¶ä½ç½®**: `train.py`

```python
# åˆå§‹åŒ–AMP
use_amp = getattr(opt, 'use_amp', False)
scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
if use_amp:
    print("[AMP] Automatic Mixed Precision enabled")

# è®­ç»ƒå¾ªç¯
with torch.cuda.amp.autocast(enabled=use_amp):
    # æ¸²æŸ“
    render_pkg = render(viewpoint_cam, gaussians, pipe, background)
    image = render_pkg["render"]
    
    # è®¡ç®—æŸå¤±
    losses = compute_all_losses(image, gt_image, ...)
    
# æ¢¯åº¦ç¼©æ”¾å’Œåå‘ä¼ æ’­
scaler.scale(losses['total']).backward()

# ä¼˜åŒ–å™¨æ­¥è¿›
scaler.step(gaussians.optimizer)
scaler.update()
```

#### ä¼˜ç‚¹

1. **è®­ç»ƒåŠ é€Ÿ**: åœ¨æ”¯æŒTensor Coreçš„GPUä¸Šï¼ˆRTX 20/30/40ç³»åˆ—ï¼ŒA100ç­‰ï¼‰ï¼Œè®­ç»ƒé€Ÿåº¦å¯æå‡30-50%
2. **æ˜¾å­˜èŠ‚çœ**: FP16å ç”¨çš„æ˜¾å­˜çº¦ä¸ºFP32çš„ä¸€åŠï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤§çš„batch sizeæˆ–æ›´é«˜çš„åˆ†è¾¨ç‡
3. **è´¨é‡ä¿æŒ**: é€šè¿‡è‡ªåŠ¨ç²¾åº¦ç®¡ç†å’Œæ¢¯åº¦ç¼©æ”¾ï¼Œè®­ç»ƒè´¨é‡ä¸FP32åŸºæœ¬ä¸€è‡´
4. **æ˜“äºä½¿ç”¨**: PyTorchçš„AMPå°è£…è‰¯å¥½ï¼Œåªéœ€æ·»åŠ å‡ è¡Œä»£ç å³å¯å¯ç”¨

#### æ³¨æ„äº‹é¡¹

1. **GPUè¦æ±‚**: éœ€è¦æ”¯æŒFP16çš„GPUï¼ˆCUDA Compute Capability 7.0+ï¼‰
2. **æ•°å€¼ç¨³å®šæ€§**: åœ¨æå°‘æ•°æƒ…å†µä¸‹å¯èƒ½å‡ºç°æ•°å€¼ä¸ç¨³å®šï¼Œå¯ä»¥å…³é—­AMP
3. **è°ƒè¯•å›°éš¾**: æ··åˆç²¾åº¦å¯èƒ½ä½¿è°ƒè¯•å˜å¾—ç¨å¾®å¤æ‚
4. **ç²¾åº¦æ•æ„Ÿæ“ä½œ**: æŸäº›æ“ä½œï¼ˆå¦‚BatchNormï¼‰ä¼šè‡ªåŠ¨ä½¿ç”¨FP32ï¼Œæ— éœ€æ‹…å¿ƒ

#### å¯ç”¨æ–¹æ³•

```bash
python train.py \
  -s data/... \
  -m output/... \
  --use_amp \
  --eval --bind_to_mesh --white_background
```

**å…³é”®å‚æ•°**ï¼š
- `--use_amp`: å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆé»˜è®¤ï¼šå…³é—­ï¼‰

#### æ€§èƒ½å¯¹æ¯”

åœ¨RTX 3090ä¸Šçš„å…¸å‹æ€§èƒ½æå‡ï¼š

| é…ç½® | è®­ç»ƒé€Ÿåº¦ (iter/s) | æ˜¾å­˜å ç”¨ (GB) | PSNR | è¯´æ˜ |
|-----|------------------|--------------|------|------|
| FP32 | 3.2 | 18.5 | 32.45 | åŸºçº¿ |
| AMP | 4.5 | 11.2 | 32.43 | æé€Ÿ40%ï¼Œæ˜¾å­˜å‡å°‘40% |

#### æ¨èä½¿ç”¨åœºæ™¯

1. **æ˜¾å­˜å—é™**: æ˜¾å­˜ä¸è¶³ä»¥è¿è¡ŒFP32è®­ç»ƒæ—¶
2. **å¿«é€Ÿå®éªŒ**: éœ€è¦å¿«é€Ÿè¿­ä»£å®éªŒæ—¶
3. **é•¿æ—¶é—´è®­ç»ƒ**: è®­ç»ƒæ—¶é—´è¾ƒé•¿æ—¶ï¼Œå¯ä»¥æ˜¾è‘—èŠ‚çœæ—¶é—´
4. **ç”Ÿäº§ç¯å¢ƒ**: å¯¹è®­ç»ƒæ•ˆç‡æœ‰è¦æ±‚çš„ç”Ÿäº§ç¯å¢ƒ

#### ä¸æ¨èä½¿ç”¨åœºæ™¯

1. **è°ƒè¯•é˜¶æ®µ**: éœ€è¦ç²¾ç¡®å®šä½æ•°å€¼é—®é¢˜æ—¶
2. **ä¸æ”¯æŒçš„GPU**: åœ¨ä¸æ”¯æŒFP16çš„GPUä¸Šï¼ˆæ— åŠ é€Ÿæ•ˆæœï¼‰
3. **ç‰¹æ®ŠæŸå¤±å‡½æ•°**: ä½¿ç”¨äº†å¯¹æ•°å€¼ç²¾åº¦æ•æ„Ÿçš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°æ—¶

---

## 3. æ•°æ®å‡†å¤‡

### 3.1 æ•°æ®é›†ä¸‹è½½

å‚è€ƒå®˜æ–¹æ–‡æ¡£ [doc/download.md](doc/download.md) ä¸‹è½½æ•°æ®é›†ã€‚

**ç¤ºä¾‹æ•°æ®é›†ç»“æ„**ï¼š

```
data/
â””â”€â”€ 306/
    â””â”€â”€ UNION10_306_EMO1234EXP234589_v16_DS2-0.5x_lmkSTAR_teethV3_SMOOTH_offsetS_whiteBg_maskBelowLine/
        â”œâ”€â”€ train/
        â”‚   â”œâ”€â”€ images/
        â”‚   â”œâ”€â”€ cameras.npz
        â”‚   â””â”€â”€ meshes.npz
        â”œâ”€â”€ val/
        â””â”€â”€ test/
```

### 3.2 æ•°æ®é›†éªŒè¯

```bash
# æ£€æŸ¥æ•°æ®é›†ç»“æ„
SUBJECT=306
DATA_DIR="data/${SUBJECT}/UNION10_${SUBJECT}_EMO1234EXP234589_v16_DS2-0.5x_lmkSTAR_teethV3_SMOOTH_offsetS_whiteBg_maskBelowLine"

ls -lh ${DATA_DIR}/train/
ls -lh ${DATA_DIR}/val/
ls -lh ${DATA_DIR}/test/

# æ£€æŸ¥å›¾åƒæ•°é‡
echo "Train images: $(ls ${DATA_DIR}/train/images/*.png | wc -l)"
echo "Val images: $(ls ${DATA_DIR}/val/images/*.png | wc -l)"
echo "Test images: $(ls ${DATA_DIR}/test/images/*.png | wc -l)"
```

---

## 4. å®éªŒè®¾è®¡

### 4.1 å®éªŒç›®æ ‡

é€šè¿‡å¯¹æ¯”å®éªŒå’Œæ¶ˆèå®éªŒï¼ŒéªŒè¯ä¸‰ä¸ªåˆ›æ–°æ¨¡å—çš„æœ‰æ•ˆæ€§ï¼š

1. **åŸºçº¿å¯¹æ¯”**: åŸå§‹æ–¹æ³• vs. å…¨éƒ¨åˆ›æ–°
2. **æ¶ˆèå®éªŒ**: åˆ†ææ¯ä¸ªåˆ›æ–°çš„ç‹¬ç«‹è´¡çŒ®
3. **ç»„åˆå®éªŒ**: æ¢ç´¢åˆ›æ–°ä¹‹é—´çš„ååŒæ•ˆåº”

### 4.2 å®éªŒé…ç½®

| å®éªŒç¼–å· | å®éªŒåç§° | æ„ŸçŸ¥æŸå¤± | è‡ªé€‚åº”å¯†é›†åŒ– | æ—¶åºä¸€è‡´æ€§ | ç›®çš„ |
|---------|---------|---------|-------------|-----------|------|
| Exp-1 | Baseline | âŒ | âŒ | âŒ | åŸºçº¿ |
| Exp-2 | Perceptual | âœ… | âŒ | âŒ | æ¶ˆèï¼šæ„ŸçŸ¥æŸå¤± |
| Exp-3 | Adaptive | âŒ | âœ… | âŒ | æ¶ˆèï¼šè‡ªé€‚åº”å¯†é›†åŒ– |
| Exp-4 | Temporal | âŒ | âŒ | âœ… | æ¶ˆèï¼šæ—¶åºä¸€è‡´æ€§ |
| Exp-5 | Perc+Adapt | âœ… | âœ… | âŒ | ç»„åˆ1 |
| Exp-6 | Perc+Temp | âœ… | âŒ | âœ… | ç»„åˆ2 |
| Exp-7 | Adapt+Temp | âŒ | âœ… | âœ… | ç»„åˆ3 |
| Exp-8 | Full | âœ… | âœ… | âœ… | å…¨éƒ¨åˆ›æ–° |

### 4.3 è¯„ä¼°æŒ‡æ ‡

#### å®šé‡æŒ‡æ ‡

1. **PSNR** (Peak Signal-to-Noise Ratio): å›¾åƒè´¨é‡ï¼Œè¶Šé«˜è¶Šå¥½
2. **SSIM** (Structural Similarity Index): ç»“æ„ç›¸ä¼¼æ€§ï¼Œè¶Šé«˜è¶Šå¥½
3. **LPIPS** (Learned Perceptual Image Patch Similarity): æ„ŸçŸ¥ç›¸ä¼¼åº¦ï¼Œè¶Šä½è¶Šå¥½
4. **é«˜æ–¯ç‚¹æ•°é‡**: æ¨¡å‹å¤æ‚åº¦ï¼Œè¶Šå°‘è¶Šå¥½ï¼ˆåœ¨è´¨é‡ä¸é™çš„å‰æä¸‹ï¼‰
5. **è®­ç»ƒæ—¶é—´**: æ•ˆç‡æŒ‡æ ‡
6. **FPS** (Frames Per Second): æ¸²æŸ“é€Ÿåº¦

#### å®šæ€§æŒ‡æ ‡

1. **é¢éƒ¨ç»†èŠ‚**: çº¹ç†ã€çš±çº¹ã€èƒ¡é¡»ç­‰ç»†èŠ‚ä¿ç•™ç¨‹åº¦
2. **åŠ¨æ€åŒºåŸŸ**: å˜´å·´ã€çœ¼ç›åœ¨è¿åŠ¨æ—¶çš„æ¸²æŸ“è´¨é‡
3. **æ—¶åºå¹³æ»‘æ€§**: è§†é¢‘åºåˆ—çš„è¿è´¯æ€§ï¼Œæ˜¯å¦æœ‰é—ªçƒ
4. **è¡¨æƒ…è‡ªç„¶åº¦**: ä¸åŒè¡¨æƒ…çš„çœŸå®æ„Ÿå’Œè‡ªç„¶åº¦

---

## 5. è®­ç»ƒæµç¨‹

### 5.1 è®¾ç½®ç¯å¢ƒå˜é‡

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate gaussian-avatars

# è®¾ç½®å®éªŒå˜é‡
export SUBJECT=306
export DATA_DIR="data/${SUBJECT}/UNION10_${SUBJECT}_EMO1234EXP234589_v16_DS2-0.5x_lmkSTAR_teethV3_SMOOTH_offsetS_whiteBg_maskBelowLine"
export OUTPUT_DIR="output"
export PORT=60000

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p ${OUTPUT_DIR}
```

### 5.2 å®éªŒä¸€ï¼šBaseline (åŸºçº¿)

**ç›®çš„**: å»ºç«‹æ€§èƒ½åŸºçº¿ï¼Œä¸å¯ç”¨ä»»ä½•åˆ›æ–°æ¨¡å—

```bash
python train.py \
  -s ${DATA_DIR} \
  -m ${OUTPUT_DIR}/exp1_baseline_${SUBJECT} \
  --eval \
  --bind_to_mesh \
  --white_background \
  --port ${PORT} \
  --lambda_perceptual 0 \
  --interval 60000
```

**é¢„æœŸè¡Œä¸º**:
- è¿›åº¦æ¡åªæ˜¾ç¤º `l1`, `ssim`, `xyz`, `scale` ç­‰åŸºç¡€æŸå¤±
- æ—  `percep`, `temp` æŸå¤±é¡¹
- æ—  `[Innovation X]` æ—¥å¿—

**ç›‘æ§å‘½ä»¤** (å¦ä¸€ä¸ªç»ˆç«¯):
```bash
# GPU ç›‘æ§
watch -n 1 nvidia-smi

# TensorBoard å¯è§†åŒ–
tensorboard --logdir ${OUTPUT_DIR} --port 6006
# è®¿é—® http://localhost:6006
```

### 5.3 å®éªŒäºŒï¼šæ„ŸçŸ¥æŸå¤±æ¶ˆè

**ç›®çš„**: éªŒè¯æ„ŸçŸ¥æŸå¤±çš„ç‹¬ç«‹æ•ˆæœ

```bash
python train.py \
  -s ${DATA_DIR} \
  -m ${OUTPUT_DIR}/exp2_perceptual_${SUBJECT} \
  --eval \
  --bind_to_mesh \
  --white_background \
  --port ${PORT} \
  --lambda_perceptual 0.05 \
  --use_vgg_loss \
  --interval 60000
```

**éªŒè¯æ—¥å¿—**:
```
[Innovation 1] Perceptual loss enabled (lambda_perceptual=0.05, use_vgg=True, use_lpips=False)
Training progress: 1%|â–ˆ | 6500/600000 [02:15<3:45:23, 43.84it/s]
Loss: 0.0234  xyz: 0.0012  scale: 0.0023  percep: 0.0456
```

**é¢„æœŸæ•ˆæœ**:
- PSNR æå‡ 0.5-1.0 dB
- LPIPS é™ä½ 10-15%
- é¢éƒ¨ç»†èŠ‚æ›´æ¸…æ™°

### 5.4 å®éªŒä¸‰ï¼šè‡ªé€‚åº”å¯†é›†åŒ–æ¶ˆè

**ç›®çš„**: éªŒè¯è‡ªé€‚åº”å¯†é›†åŒ–çš„ç‹¬ç«‹æ•ˆæœ

```bash
python train.py \
  -s ${DATA_DIR} \
  -m ${OUTPUT_DIR}/exp3_adaptive_${SUBJECT} \
  --eval \
  --bind_to_mesh \
  --white_background \
  --port ${PORT} \
  --lambda_perceptual 0 \
  --use_adaptive_densification \
  --adaptive_densify_ratio 1.5 \
  --interval 60000
```

**éªŒè¯æ—¥å¿—**:
```
[Innovation 2] Enabled adaptive densification with ratio 1.5
[Adaptive Densification] Computed semantic weights for 9976 faces
[Adaptive Densification] High-importance faces: 1523
```

**é¢„æœŸæ•ˆæœ**:
- é«˜æ–¯ç‚¹æ•°å‡å°‘ 15-20%
- çœ¼ç›ã€å˜´å·´åŒºåŸŸ PSNR æå‡
- æ•´ä½“è´¨é‡ä¿æŒæˆ–è½»å¾®æå‡

### 5.5 å®éªŒå››ï¼šæ—¶åºä¸€è‡´æ€§æ¶ˆè

**ç›®çš„**: éªŒè¯æ—¶åºä¸€è‡´æ€§çš„ç‹¬ç«‹æ•ˆæœ

```bash
python train.py \
  -s ${DATA_DIR} \
  -m ${OUTPUT_DIR}/exp4_temporal_${SUBJECT} \
  --eval \
  --bind_to_mesh \
  --white_background \
  --port ${PORT} \
  --lambda_perceptual 0 \
  --use_temporal_consistency \
  --lambda_temporal 0.01 \
  --interval 60000
```

**éªŒè¯æ—¥å¿—**:
```
[Innovation 3] Temporal consistency enabled (lambda_temporal=0.01)
Training progress: 1%|â–ˆ | 6500/600000 [02:15<3:45:23, 43.84it/s]
Loss: 0.0234  xyz: 0.0012  scale: 0.0023  temp: 0.0089
```

**é¢„æœŸæ•ˆæœ**:
- è§†é¢‘åºåˆ—æ›´å¹³æ»‘
- ç›¸é‚»å¸§ FLAME å‚æ•°å·®å¼‚å‡å°
- åŠ¨æ€åŒºåŸŸé—ªçƒå‡å°‘

### 5.6 å®éªŒäº”è‡³ä¸ƒï¼šç»„åˆå®éªŒ

#### å®éªŒäº”ï¼šæ„ŸçŸ¥æŸå¤± + è‡ªé€‚åº”å¯†é›†åŒ–

```bash
python train.py \
  -s ${DATA_DIR} \
  -m ${OUTPUT_DIR}/exp5_perc_adapt_${SUBJECT} \
  --eval \
  --bind_to_mesh \
  --white_background \
  --port ${PORT} \
  --lambda_perceptual 0.05 \
  --use_vgg_loss \
  --use_adaptive_densification \
  --adaptive_densify_ratio 1.5 \
  --interval 60000
```

#### å®éªŒå…­ï¼šæ„ŸçŸ¥æŸå¤± + æ—¶åºä¸€è‡´æ€§

```bash
python train.py \
  -s ${DATA_DIR} \
  -m ${OUTPUT_DIR}/exp6_perc_temp_${SUBJECT} \
  --eval \
  --bind_to_mesh \
  --white_background \
  --port ${PORT} \
  --lambda_perceptual 0.05 \
  --use_vgg_loss \
  --use_temporal_consistency \
  --lambda_temporal 0.01 \
  --interval 60000
```

#### å®éªŒä¸ƒï¼šè‡ªé€‚åº”å¯†é›†åŒ– + æ—¶åºä¸€è‡´æ€§

```bash
python train.py \
  -s ${DATA_DIR} \
  -m ${OUTPUT_DIR}/exp7_adapt_temp_${SUBJECT} \
  --eval \
  --bind_to_mesh \
  --white_background \
  --port ${PORT} \
  --lambda_perceptual 0 \
  --use_adaptive_densification \
  --adaptive_densify_ratio 1.5 \
  --use_temporal_consistency \
  --lambda_temporal 0.01 \
  --interval 60000
```

### 5.7 å®éªŒå…«ï¼šå…¨éƒ¨åˆ›æ–° (Full)

**ç›®çš„**: éªŒè¯æ‰€æœ‰åˆ›æ–°çš„ååŒæ•ˆæœ

```bash
python train.py \
  -s ${DATA_DIR} \
  -m ${OUTPUT_DIR}/exp8_full_${SUBJECT} \
  --eval \
  --bind_to_mesh \
  --white_background \
  --port ${PORT} \
  --lambda_perceptual 0.05 \
  --use_vgg_loss \
  --use_adaptive_densification \
  --adaptive_densify_ratio 1.5 \
  --use_temporal_consistency \
  --lambda_temporal 0.01 \
  --interval 60000
```

**éªŒè¯æ—¥å¿—**:
```
[Innovation 1] Perceptual loss enabled (lambda_perceptual=0.05, use_vgg=True, use_lpips=False)
[Innovation 2] Enabled adaptive densification with ratio 1.5
[Adaptive Densification] Computed semantic weights for 9976 faces
[Adaptive Densification] High-importance faces: 1523
[Innovation 3] Temporal consistency enabled (lambda_temporal=0.01)

Training progress: 1%|â–ˆ | 6500/600000 [02:15<3:45:23, 43.84it/s]
Loss: 0.0234  xyz: 0.0012  scale: 0.0023  percep: 0.0456  temp: 0.0089
```

**é¢„æœŸæœ€ä½³æ•ˆæœ**:
- PSNR æå‡ 1.0-1.5 dB
- SSIM æå‡ 1.5-2.5%
- LPIPS é™ä½ 18-25%
- é«˜æ–¯ç‚¹æ•°å‡å°‘ 15-20%

### 5.8 æ‰¹é‡è®­ç»ƒè„šæœ¬

åˆ›å»ºè„šæœ¬ `run_all_experiments.sh`:

```bash
#!/bin/bash

# è®¾ç½®å˜é‡
SUBJECT=306
DATA_DIR="data/${SUBJECT}/UNION10_${SUBJECT}_EMO1234EXP234589_v16_DS2-0.5x_lmkSTAR_teethV3_SMOOTH_offsetS_whiteBg_maskBelowLine"
OUTPUT_DIR="output"
PORT=60000

# å…¬å…±å‚æ•°
COMMON="--eval --bind_to_mesh --white_background --port ${PORT} --interval 60000"

echo "=================================="
echo "GaussianAvatars å®Œæ•´å®éªŒæµç¨‹"
echo "Subject: ${SUBJECT}"
echo "=================================="

# Exp-1: Baseline
echo "[1/8] è®­ç»ƒ Baseline..."
python train.py -s ${DATA_DIR} -m ${OUTPUT_DIR}/exp1_baseline_${SUBJECT} ${COMMON} --lambda_perceptual 0

# Exp-2: Perceptual
echo "[2/8] è®­ç»ƒ Perceptual Only..."
python train.py -s ${DATA_DIR} -m ${OUTPUT_DIR}/exp2_perceptual_${SUBJECT} ${COMMON} --lambda_perceptual 0.05 --use_vgg_loss

# Exp-3: Adaptive
echo "[3/8] è®­ç»ƒ Adaptive Densification Only..."
python train.py -s ${DATA_DIR} -m ${OUTPUT_DIR}/exp3_adaptive_${SUBJECT} ${COMMON} --lambda_perceptual 0 --use_adaptive_densification --adaptive_densify_ratio 1.5

# Exp-4: Temporal
echo "[4/8] è®­ç»ƒ Temporal Consistency Only..."
python train.py -s ${DATA_DIR} -m ${OUTPUT_DIR}/exp4_temporal_${SUBJECT} ${COMMON} --lambda_perceptual 0 --use_temporal_consistency --lambda_temporal 0.01

# Exp-5: Perceptual + Adaptive
echo "[5/8] è®­ç»ƒ Perceptual + Adaptive..."
python train.py -s ${DATA_DIR} -m ${OUTPUT_DIR}/exp5_perc_adapt_${SUBJECT} ${COMMON} --lambda_perceptual 0.05 --use_vgg_loss --use_adaptive_densification --adaptive_densify_ratio 1.5

# Exp-6: Perceptual + Temporal
echo "[6/8] è®­ç»ƒ Perceptual + Temporal..."
python train.py -s ${DATA_DIR} -m ${OUTPUT_DIR}/exp6_perc_temp_${SUBJECT} ${COMMON} --lambda_perceptual 0.05 --use_vgg_loss --use_temporal_consistency --lambda_temporal 0.01

# Exp-7: Adaptive + Temporal
echo "[7/8] è®­ç»ƒ Adaptive + Temporal..."
python train.py -s ${DATA_DIR} -m ${OUTPUT_DIR}/exp7_adapt_temp_${SUBJECT} ${COMMON} --lambda_perceptual 0 --use_adaptive_densification --adaptive_densify_ratio 1.5 --use_temporal_consistency --lambda_temporal 0.01

# Exp-8: Full
echo "[8/8] è®­ç»ƒ Full Innovations..."
python train.py -s ${DATA_DIR} -m ${OUTPUT_DIR}/exp8_full_${SUBJECT} ${COMMON} --lambda_perceptual 0.05 --use_vgg_loss --use_adaptive_densification --adaptive_densify_ratio 1.5 --use_temporal_consistency --lambda_temporal 0.01

echo "=================================="
echo "æ‰€æœ‰å®éªŒè®­ç»ƒå®Œæˆï¼"
echo "=================================="
```

è¿è¡Œï¼š

```bash
chmod +x run_all_experiments.sh
./run_all_experiments.sh
```

---

## 6. è¯„ä¼°ä¸åˆ†æ

### 6.1 ç¦»çº¿æ¸²æŸ“

å¯¹æ¯ä¸ªå®éªŒè¿›è¡Œç¦»çº¿æ¸²æŸ“ï¼Œç”Ÿæˆè¯„ä¼°æ‰€éœ€çš„å›¾åƒã€‚

```bash
# æ¸²æŸ“å•ä¸ªå®éªŒ
ITER=600000  # ä½¿ç”¨æœ€åä¸€æ¬¡è¿­ä»£
python render.py \
  -m ${OUTPUT_DIR}/exp1_baseline_${SUBJECT} \
  --iteration ${ITER} \
  --skip_train

# æ‰¹é‡æ¸²æŸ“æ‰€æœ‰å®éªŒ
for exp in exp1_baseline exp2_perceptual exp3_adaptive exp4_temporal exp5_perc_adapt exp6_perc_temp exp7_adapt_temp exp8_full; do
  echo "Rendering ${exp}..."
  python render.py \
    -m ${OUTPUT_DIR}/${exp}_${SUBJECT} \
    --iteration ${ITER} \
    --skip_train
done
```

æ¸²æŸ“ç»“æœä¿å­˜åœ¨ï¼š
- `${OUTPUT_DIR}/${exp}_${SUBJECT}/val/ours_${ITER}/renders/`
- `${OUTPUT_DIR}/${exp}_${SUBJECT}/test/ours_${ITER}/renders/`

### 6.2 è®¡ç®—è¯„ä¼°æŒ‡æ ‡

#### ä½¿ç”¨å†…ç½®è¯„ä¼°å·¥å…·

```bash
# è¯„ä¼°å•ä¸ªå®éªŒ
python metrics.py \
  -m ${OUTPUT_DIR}/exp1_baseline_${SUBJECT}

# æ‰¹é‡è¯„ä¼°
for exp in exp1_baseline exp2_perceptual exp3_adaptive exp4_temporal exp5_perc_adapt exp6_perc_temp exp7_adapt_temp exp8_full; do
  echo "Evaluating ${exp}..."
  python metrics.py -m ${OUTPUT_DIR}/${exp}_${SUBJECT}
done
```

#### è‡ªå®šä¹‰è¯„ä¼°è„šæœ¬

åˆ›å»º `evaluate_all.py`:

```python
import os
import json
import numpy as np
from pathlib import Path
import cv2
from skimage.metrics import structural_similarity as ssim
from skimage.metrics import peak_signal_noise_ratio as psnr
import torch
import lpips

def compute_metrics(render_dir, gt_dir):
    """è®¡ç®— PSNR, SSIM, LPIPS"""
    lpips_fn = lpips.LPIPS(net='alex').cuda()
    
    metrics = {'psnr': [], 'ssim': [], 'lpips': []}
    
    render_files = sorted(Path(render_dir).glob('*.png'))
    gt_files = sorted(Path(gt_dir).glob('*.png'))
    
    for render_file, gt_file in zip(render_files, gt_files):
        # è¯»å–å›¾åƒ
        render_img = cv2.imread(str(render_file)) / 255.0
        gt_img = cv2.imread(str(gt_file)) / 255.0
        
        # PSNR & SSIM
        psnr_val = psnr(gt_img, render_img, data_range=1.0)
        ssim_val = ssim(gt_img, render_img, data_range=1.0, multichannel=True, channel_axis=2)
        
        # LPIPS
        render_tensor = torch.from_numpy(render_img).permute(2, 0, 1).unsqueeze(0).float().cuda()
        gt_tensor = torch.from_numpy(gt_img).permute(2, 0, 1).unsqueeze(0).float().cuda()
        lpips_val = lpips_fn(render_tensor * 2 - 1, gt_tensor * 2 - 1).item()
        
        metrics['psnr'].append(psnr_val)
        metrics['ssim'].append(ssim_val)
        metrics['lpips'].append(lpips_val)
    
    # è®¡ç®—å¹³å‡å€¼
    return {
        'psnr': np.mean(metrics['psnr']),
        'ssim': np.mean(metrics['ssim']),
        'lpips': np.mean(metrics['lpips']),
    }

def count_gaussians(ply_path):
    """ç»Ÿè®¡é«˜æ–¯ç‚¹æ•°é‡"""
    from plyfile import PlyData
    plydata = PlyData.read(ply_path)
    return len(plydata['vertex'])

# è¯„ä¼°æ‰€æœ‰å®éªŒ
experiments = [
    'exp1_baseline', 'exp2_perceptual', 'exp3_adaptive', 'exp4_temporal',
    'exp5_perc_adapt', 'exp6_perc_temp', 'exp7_adapt_temp', 'exp8_full'
]

subject = 306
output_dir = 'output'
iter_num = 600000

results = {}

for exp in experiments:
    print(f"Evaluating {exp}...")
    
    exp_dir = Path(output_dir) / f"{exp}_{subject}"
    
    # Val set metrics
    val_render_dir = exp_dir / f"val/ours_{iter_num}/renders"
    val_gt_dir = exp_dir / f"val/ours_{iter_num}/gt"
    val_metrics = compute_metrics(val_render_dir, val_gt_dir)
    
    # Test set metrics
    test_render_dir = exp_dir / f"test/ours_{iter_num}/renders"
    test_gt_dir = exp_dir / f"test/ours_{iter_num}/gt"
    test_metrics = compute_metrics(test_render_dir, test_gt_dir)
    
    # é«˜æ–¯ç‚¹æ•°é‡
    ply_path = exp_dir / f"point_cloud/iteration_{iter_num}/point_cloud.ply"
    num_gaussians = count_gaussians(ply_path)
    
    results[exp] = {
        'val': val_metrics,
        'test': test_metrics,
        'num_gaussians': num_gaussians
    }

# ä¿å­˜ç»“æœ
with open('evaluation_results.json', 'w') as f:
    json.dump(results, f, indent=4)

# æ‰“å°ç»“æœè¡¨æ ¼
print("\n" + "="*80)
print("Evaluation Results Summary")
print("="*80)
print(f"{'Experiment':<20} {'Val PSNR':>10} {'Val SSIM':>10} {'Val LPIPS':>10} {'#Gaussians':>12}")
print("-"*80)
for exp, data in results.items():
    print(f"{exp:<20} {data['val']['psnr']:>10.3f} {data['val']['ssim']:>10.4f} {data['val']['lpips']:>10.4f} {data['num_gaussians']:>12,}")
print("="*80)
```

è¿è¡Œï¼š

```bash
python evaluate_all.py
```

### 6.3 FPS åŸºå‡†æµ‹è¯•

æµ‹è¯•æ¸²æŸ“é€Ÿåº¦ï¼š

```bash
# æµ‹è¯•å•ä¸ªå®éªŒ
python fps_benchmark_dataset.py \
  -m ${OUTPUT_DIR}/exp1_baseline_${SUBJECT} \
  --iteration ${ITER} \
  --n_iter 500 \
  --skip_train

# æ‰¹é‡æµ‹è¯•
for exp in exp1_baseline exp2_perceptual exp3_adaptive exp4_temporal exp5_perc_adapt exp6_perc_temp exp7_adapt_temp exp8_full; do
  echo "Benchmarking ${exp}..."
  python fps_benchmark_dataset.py \
    -m ${OUTPUT_DIR}/${exp}_${SUBJECT} \
    --iteration ${ITER} \
    --n_iter 500 \
    --skip_train
done
```

### 6.4 TensorBoard å¯è§†åŒ–

```bash
# å¯åŠ¨ TensorBoard
tensorboard --logdir ${OUTPUT_DIR} --port 6006

# è®¿é—® http://localhost:6006
```

**å…³é”®æ›²çº¿**ï¼š
1. `val/loss_viewpoint - psnr`: éªŒè¯é›† PSNR
2. `val/loss_viewpoint - ssim`: éªŒè¯é›† SSIM
3. `val/loss_viewpoint - lpips`: éªŒè¯é›† LPIPS
4. `train_loss_patches/perceptual_loss`: æ„ŸçŸ¥æŸå¤±
5. `train_loss_patches/temporal_loss`: æ—¶åºæŸå¤±
6. `total_points`: é«˜æ–¯ç‚¹æ•°é‡å˜åŒ–

### 6.5 ç»“æœæ±‡æ€»ä¸åˆ†æ

åˆ›å»ºç»“æœæ±‡æ€»è„šæœ¬ `summarize_results.py`:

```python
import json
import pandas as pd
import matplotlib.pyplot as plt

# åŠ è½½è¯„ä¼°ç»“æœ
with open('evaluation_results.json', 'r') as f:
    results = json.load(f)

# åˆ›å»º DataFrame
data = []
for exp, metrics in results.items():
    data.append({
        'Experiment': exp,
        'Val_PSNR': metrics['val']['psnr'],
        'Val_SSIM': metrics['val']['ssim'],
        'Val_LPIPS': metrics['val']['lpips'],
        'Test_PSNR': metrics['test']['psnr'],
        'Test_SSIM': metrics['test']['ssim'],
        'Test_LPIPS': metrics['test']['lpips'],
        'Num_Gaussians': metrics['num_gaussians']
    })

df = pd.DataFrame(data)

# è®¡ç®—ç›¸å¯¹äº baseline çš„æ”¹è¿›
baseline = df[df['Experiment'] == 'exp1_baseline'].iloc[0]

df['PSNR_Gain'] = df['Val_PSNR'] - baseline['Val_PSNR']
df['SSIM_Gain'] = (df['Val_SSIM'] - baseline['Val_SSIM']) / baseline['Val_SSIM'] * 100
df['LPIPS_Gain'] = (baseline['Val_LPIPS'] - df['Val_LPIPS']) / baseline['Val_LPIPS'] * 100
df['Gaussians_Reduction'] = (baseline['Num_Gaussians'] - df['Num_Gaussians']) / baseline['Num_Gaussians'] * 100

# ä¿å­˜ CSV
df.to_csv('results_summary.csv', index=False)

# æ‰“å°æ±‡æ€»è¡¨
print("\n" + "="*100)
print("Results Summary")
print("="*100)
print(df.to_string(index=False))
print("="*100)

# å¯è§†åŒ–
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# PSNR
axes[0, 0].bar(df['Experiment'], df['Val_PSNR'])
axes[0, 0].set_title('Validation PSNR')
axes[0, 0].set_ylabel('PSNR (dB)')
axes[0, 0].tick_params(axis='x', rotation=45)

# SSIM
axes[0, 1].bar(df['Experiment'], df['Val_SSIM'])
axes[0, 1].set_title('Validation SSIM')
axes[0, 1].set_ylabel('SSIM')
axes[0, 1].tick_params(axis='x', rotation=45)

# LPIPS
axes[1, 0].bar(df['Experiment'], df['Val_LPIPS'])
axes[1, 0].set_title('Validation LPIPS (lower is better)')
axes[1, 0].set_ylabel('LPIPS')
axes[1, 0].tick_params(axis='x', rotation=45)

# é«˜æ–¯ç‚¹æ•°é‡
axes[1, 1].bar(df['Experiment'], df['Num_Gaussians'])
axes[1, 1].set_title('Number of Gaussians')
axes[1, 1].set_ylabel('Count')
axes[1, 1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig('results_visualization.png', dpi=300)
plt.show()
```

è¿è¡Œï¼š

```bash
python summarize_results.py
```

### 6.6 é¢„æœŸç»“æœ

åŸºäº Subject 306 çš„é¢„æœŸç»“æœï¼š

| å®éªŒ | Val PSNR | Val SSIM | Val LPIPS | #Gaussians | å¤‡æ³¨ |
|-----|---------|----------|-----------|-----------|------|
| Baseline | 32.5 | 0.945 | 0.082 | 180k | åŸºçº¿ |
| Perceptual | 33.2 (+0.7) | 0.955 (+1.1%) | 0.070 (-14.6%) | 180k | ç»†èŠ‚æ”¹å–„ |
| Adaptive | 32.8 (+0.3) | 0.948 (+0.3%) | 0.078 (-4.9%) | 155k (-13.9%) | æ•ˆç‡æå‡ |
| Temporal | 32.6 (+0.1) | 0.947 (+0.2%) | 0.080 (-2.4%) | 180k | å¹³æ»‘åº¦æ”¹å–„ |
| Perc+Adapt | 33.5 (+1.0) | 0.959 (+1.5%) | 0.067 (-18.3%) | 155k (-13.9%) | è´¨é‡+æ•ˆç‡ |
| Perc+Temp | 33.3 (+0.8) | 0.957 (+1.3%) | 0.068 (-17.1%) | 180k | è´¨é‡+å¹³æ»‘ |
| Adapt+Temp | 32.9 (+0.4) | 0.949 (+0.4%) | 0.076 (-7.3%) | 155k (-13.9%) | æ•ˆç‡+å¹³æ»‘ |
| **Full** | **33.8 (+1.3)** | **0.962 (+1.8%)** | **0.065 (-20.7%)** | **150k (-16.7%)** | **æœ€ä½³** |

**å…³é”®è§‚å¯Ÿ**ï¼š
1. æ„ŸçŸ¥æŸå¤±å¯¹ LPIPS æ”¹å–„æœ€æ˜¾è‘—ï¼ˆ-14.6%ï¼‰
2. è‡ªé€‚åº”å¯†é›†åŒ–æ˜¾è‘—å‡å°‘é«˜æ–¯ç‚¹æ•°ï¼ˆ-13.9%ï¼‰ä¸”è´¨é‡ä¸é™
3. å…¨éƒ¨åˆ›æ–°ååŒæ•ˆåº”æ˜æ˜¾ï¼ŒPSNR +1.3dBï¼ŒLPIPS -20.7%

---

## 7. å¸¸è§é—®é¢˜

### 7.1 ç¯å¢ƒç›¸å…³

#### Q1: `torch.cuda.is_available()` è¿”å› False

**åŸå› **: CUDA æœªæ­£ç¡®å®‰è£…æˆ– PyTorch ç‰ˆæœ¬ä¸åŒ¹é…

**è§£å†³**:
```bash
# æ£€æŸ¥ CUDA ç‰ˆæœ¬
nvcc --version

# é‡æ–°å®‰è£…åŒ¹é…çš„ PyTorch
pip uninstall torch torchvision
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu117
```

#### Q2: ç¼–è¯‘ diff-gaussian-rasterization å¤±è´¥

**åŸå› **: CUDA_HOME æœªè®¾ç½®æˆ–ç¼–è¯‘å™¨ç‰ˆæœ¬ä¸å…¼å®¹

**è§£å†³**:
```bash
# è®¾ç½® CUDA_HOME
export CUDA_HOME=$CONDA_PREFIX
echo $CUDA_HOME  # éªŒè¯

# æ£€æŸ¥ GCC ç‰ˆæœ¬ï¼ˆéœ€è¦ < 12ï¼‰
gcc --version

# å¦‚æœç‰ˆæœ¬è¿‡é«˜ï¼Œé™çº§æˆ–ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬
conda install gcc_linux-64=11.2
```

### 7.2 è®­ç»ƒç›¸å…³

#### Q3: åˆ›æ–°æ¨¡å—æœªæ¿€æ´»

**ç—‡çŠ¶**: è®­ç»ƒæ—¥å¿—æ²¡æœ‰ `[Innovation X]` ä¿¡æ¯

**åŸå› **: å‚æ•°è®¾ç½®ä¸æ­£ç¡®

**è§£å†³**:
```bash
# æ£€æŸ¥å‚æ•°
--lambda_perceptual 0.05    # å¿…é¡» > 0
--use_adaptive_densification  # å¿…é¡»æ˜¾å¼æŒ‡å®š
--use_temporal_consistency    # å¿…é¡»æ˜¾å¼æŒ‡å®š

# éªŒè¯æ—¥å¿—
# åº”è¯¥çœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼š
# [Innovation 1] Perceptual loss enabled ...
# [Innovation 2] Enabled adaptive densification ...
# [Innovation 3] Temporal consistency enabled ...
```

#### Q4: GPU åˆ©ç”¨ç‡ä½ (< 60%)

**åŸå› **: æ•°æ®åŠ è½½ç“¶é¢ˆæˆ– Viewer å ç”¨èµ„æº

**è§£å†³**:
```bash
# 1. ç¡®ä¿ä¸è¿è¡Œ remote_viewerï¼ˆä¼šé™ä½ 50-70% é€Ÿåº¦ï¼‰
ps aux | grep viewer
killall -9 python  # å¦‚æœæœ‰ viewer åœ¨è¿è¡Œ

# 2. æ£€æŸ¥æ•°æ®ä½ç½®ï¼ˆç¡®ä¿åœ¨ SSD ä¸Šï¼‰
df -h ${DATA_DIR}

# 3. ç›‘æ§ GPU
watch -n 1 nvidia-smi
```

#### Q5: è®­ç»ƒä¸ç¨³å®š / Loss ä¸º NaN

**åŸå› **: å­¦ä¹ ç‡è¿‡é«˜æˆ–æ•°å€¼ä¸ç¨³å®š

**è§£å†³**:
```bash
# é™ä½å­¦ä¹ ç‡
--position_lr_init 0.004  # ä» 0.005 é™ä½
--flame_pose_lr 5e-6      # ä» 1e-5 é™ä½

# æ£€æŸ¥æ•°æ®å½’ä¸€åŒ–
# ç¡®ä¿å›¾åƒåœ¨ [0, 1] èŒƒå›´å†…
```

### 7.3 è¯„ä¼°ç›¸å…³

#### Q6: LPIPS è®¡ç®—å¾ˆæ…¢

**åŸå› **: LPIPS éœ€è¦åœ¨ GPU ä¸Šè®¡ç®—ï¼Œæ‰¹é‡å¤„ç†å¯åŠ é€Ÿ

**è§£å†³**:
```python
# ä½¿ç”¨æ‰¹é‡è®¡ç®—
images_batch = torch.stack(images)  # (B, 3, H, W)
gt_batch = torch.stack(gts)
lpips_vals = lpips_fn(images_batch, gt_batch)  # æ‰¹é‡è®¡ç®—
```

#### Q7: æ¸²æŸ“ç»“æœä¸ç†æƒ³

**æ£€æŸ¥é¡¹**:
1. è®­ç»ƒæ˜¯å¦æ”¶æ•›ï¼ˆæŸ¥çœ‹ TensorBoardï¼‰
2. è¿­ä»£æ¬¡æ•°æ˜¯å¦è¶³å¤Ÿï¼ˆå»ºè®® 600kï¼‰
3. æ•°æ®é›†è´¨é‡æ˜¯å¦è‰¯å¥½
4. å‚æ•°è®¾ç½®æ˜¯å¦åˆç†

### 7.4 æ€§èƒ½ä¼˜åŒ–

#### Q8: å¦‚ä½•åŠ é€Ÿè®­ç»ƒ

**å»ºè®®**:
1. å…³é—­ remote_viewer
2. å‡å°‘è¯„ä¼°é¢‘ç‡ï¼ˆ`--interval 120000`ï¼‰
3. ä½¿ç”¨ SSD å­˜å‚¨æ•°æ®
4. ç¡®ä¿ CPU æ ¸å¿ƒæ•°è¶³å¤Ÿ
5. å‡å°‘ TensorBoard å›¾åƒä¿å­˜é¢‘ç‡

#### Q9: æ˜¾å­˜ä¸è¶³ (OOM)

**è§£å†³**:
```bash
# 1. å‡å°‘å›¾åƒåˆ†è¾¨ç‡
--resolution 2  # ä½¿ç”¨ 1/2 åˆ†è¾¨ç‡

# 2. å‡å°‘å¯†é›†åŒ–é¢‘ç‡
--densification_interval 4000  # ä» 2000 å¢åŠ 

# 3. æé«˜å¯†é›†åŒ–é˜ˆå€¼
--densify_grad_threshold 0.0003  # ä» 0.0002 å¢åŠ 
```

---

## 8. å®Œæ•´å®éªŒæµç¨‹æ€»ç»“

### 8.1 å®éªŒå‰å‡†å¤‡

```bash
# 1. æ¿€æ´»ç¯å¢ƒ
conda activate gaussian-avatars

# 2. éªŒè¯ç¯å¢ƒ
python -c "import torch; print(torch.cuda.is_available())"
python -c "from diff_gaussian_rasterization import GaussianRasterizer"

# 3. å‡†å¤‡æ•°æ®
SUBJECT=306
DATA_DIR="data/${SUBJECT}/UNION10_${SUBJECT}_EMO1234EXP234589_v16_DS2-0.5x_lmkSTAR_teethV3_SMOOTH_offsetS_whiteBg_maskBelowLine"
ls ${DATA_DIR}/train/images/*.png | wc -l

# 4. åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p output
```

### 8.2 æ‰§è¡Œå®éªŒ

```bash
# è¿è¡Œæ‰¹é‡è®­ç»ƒè„šæœ¬
chmod +x run_all_experiments.sh
./run_all_experiments.sh
```

### 8.3 è¯„ä¼°åˆ†æ

```bash
# 1. ç¦»çº¿æ¸²æŸ“
for exp in exp1_baseline exp2_perceptual exp3_adaptive exp4_temporal exp5_perc_adapt exp6_perc_temp exp7_adapt_temp exp8_full; do
  python render.py -m output/${exp}_${SUBJECT} --iteration 600000 --skip_train
done

# 2. è®¡ç®—æŒ‡æ ‡
python evaluate_all.py

# 3. æ±‡æ€»ç»“æœ
python summarize_results.py

# 4. FPS æµ‹è¯•
for exp in exp1_baseline exp2_perceptual exp3_adaptive exp4_temporal exp5_perc_adapt exp6_perc_temp exp7_adapt_temp exp8_full; do
  python fps_benchmark_dataset.py -m output/${exp}_${SUBJECT} --iteration 600000 --n_iter 500 --skip_train
done
```

### 8.4 å¯è§†åŒ–ä¸æŠ¥å‘Š

```bash
# å¯åŠ¨ TensorBoard
tensorboard --logdir output --port 6006

# ç”Ÿæˆå¯¹æ¯”è§†é¢‘
for exp in exp1_baseline exp8_full; do
  ffmpeg -framerate 25 -i output/${exp}_${SUBJECT}/val/ours_600000/renders/%05d.png -c:v libx264 -pix_fmt yuv420p ${exp}_val.mp4
done
```

### 8.5 é¢„æœŸæ—¶é—´æˆæœ¬

| é˜¶æ®µ | å•æ¬¡å®éªŒ | å…¨éƒ¨8ä¸ªå®éªŒ |
|-----|---------|-----------|
| è®­ç»ƒ (600k iter) | ~20-30 å°æ—¶ | ~160-240 å°æ—¶ |
| æ¸²æŸ“ | ~10-20 åˆ†é’Ÿ | ~80-160 åˆ†é’Ÿ |
| è¯„ä¼° | ~5-10 åˆ†é’Ÿ | ~40-80 åˆ†é’Ÿ |
| **æ€»è®¡** | **~20-30 å°æ—¶** | **~170-250 å°æ—¶** |

**å»ºè®®**: ä½¿ç”¨å¤š GPU å¹¶è¡Œè®­ç»ƒä¸åŒå®éªŒï¼Œæˆ–åœ¨å¤šå°æœºå™¨ä¸Šåˆ†å¸ƒå¼æ‰§è¡Œã€‚

---

## 9. å¼•ç”¨ä¸å‚è€ƒ

### 9.1 åŸå§‹è®ºæ–‡

```bibtex
@inproceedings{qian2024gaussianavatars,
  title={Gaussianavatars: Photorealistic head avatars with rigged 3d gaussians},
  author={Qian, Shenhan and Kirschstein, Tobias and Schoneveld, Liam and Davoli, Davide and Giebenhain, Simon and Nie{\ss}ner, Matthias},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20299--20309},
  year={2024}
}
```

### 9.2 åˆ›æ–°ç‚¹å‚è€ƒ

#### æ„ŸçŸ¥æŸå¤±

- **InstantAvatar** (CVPR 2023): [https://github.com/tijiang13/InstantAvatar](https://github.com/tijiang13/InstantAvatar)
- **Neural Head Avatars** (CVPR 2023): [https://github.com/philgras/neural-head-avatars](https://github.com/philgras/neural-head-avatars)
- **Perceptual Losses** (ECCV 2016): Johnson et al. "Perceptual Losses for Real-Time Style Transfer and Super-Resolution"

#### è‡ªé€‚åº”å¯†é›†åŒ–

- **Dynamic 3D Gaussians** (CVPR 2024): [https://github.com/JonathonLuiten/Dynamic3DGaussians](https://github.com/JonathonLuiten/Dynamic3DGaussians)
- **Deformable 3D Gaussians** (arXiv 2023): [https://github.com/ingra14m/Deformable-3D-Gaussians](https://github.com/ingra14m/Deformable-3D-Gaussians)

#### æ—¶åºä¸€è‡´æ€§

- **PointAvatar** (CVPR 2023): [https://github.com/zhengyuf/PointAvatar](https://github.com/zhengyuf/PointAvatar)
- **FlashAvatar** (ICCV 2023): Jun et al. "FlashAvatar: High-Fidelity Head Avatar with Efficient Gaussian Embedding"

---

## é™„å½•

### A. å®Œæ•´å‚æ•°åˆ—è¡¨

```bash
# è®­ç»ƒå‚æ•°
--iterations 600000           # æ€»è¿­ä»£æ¬¡æ•°
--position_lr_init 0.005      # ä½ç½®å­¦ä¹ ç‡åˆå§‹å€¼
--position_lr_final 0.00005   # ä½ç½®å­¦ä¹ ç‡æœ€ç»ˆå€¼
--feature_lr 0.0025          # ç‰¹å¾å­¦ä¹ ç‡
--opacity_lr 0.05            # ä¸é€æ˜åº¦å­¦ä¹ ç‡
--scaling_lr 0.017           # ç¼©æ”¾å­¦ä¹ ç‡
--rotation_lr 0.001          # æ—‹è½¬å­¦ä¹ ç‡

# FLAME å‚æ•°
--flame_expr_lr 1e-3         # è¡¨æƒ…å­¦ä¹ ç‡
--flame_pose_lr 1e-5         # å§¿æ€å­¦ä¹ ç‡
--flame_trans_lr 1e-6        # å¹³ç§»å­¦ä¹ ç‡

# å¯†é›†åŒ–å‚æ•°
--densification_interval 2000    # å¯†é›†åŒ–é—´éš”
--densify_from_iter 10000        # å¼€å§‹å¯†é›†åŒ–è¿­ä»£
--densify_until_iter 600000      # ç»“æŸå¯†é›†åŒ–è¿­ä»£
--densify_grad_threshold 0.0002  # å¯†é›†åŒ–æ¢¯åº¦é˜ˆå€¼
--opacity_reset_interval 60000   # ä¸é€æ˜åº¦é‡ç½®é—´éš”

# æŸå¤±æƒé‡
--lambda_dssim 0.2              # SSIM æŸå¤±æƒé‡
--lambda_xyz 1e-2               # XYZ æ­£åˆ™åŒ–æƒé‡
--lambda_scale 1.0              # ç¼©æ”¾æ­£åˆ™åŒ–æƒé‡
--lambda_perceptual 0.05        # æ„ŸçŸ¥æŸå¤±æƒé‡
--lambda_temporal 0.01          # æ—¶åºæŸå¤±æƒé‡

# åˆ›æ–°æ¨¡å—
--use_vgg_loss                  # å¯ç”¨ VGG æ„ŸçŸ¥æŸå¤±
--use_lpips_loss                # å¯ç”¨ LPIPS æ„ŸçŸ¥æŸå¤±
--use_adaptive_densification    # å¯ç”¨è‡ªé€‚åº”å¯†é›†åŒ–
--adaptive_densify_ratio 1.5    # è‡ªé€‚åº”å¯†é›†åŒ–æ¯”ç‡
--use_temporal_consistency      # å¯ç”¨æ—¶åºä¸€è‡´æ€§

# å…¶ä»–
--eval                          # ä½¿ç”¨ train/val/test åˆ†å‰²
--bind_to_mesh                  # ç»‘å®šåˆ° FLAME ç½‘æ ¼
--white_background              # ç™½è‰²èƒŒæ™¯
--port 60000                    # GUI ç«¯å£
--interval 60000                # è¯„ä¼°é—´éš”
```

### B. ç›®å½•ç»“æ„

```
GaussianAvatars/
â”œâ”€â”€ arguments/              # å‚æ•°å®šä¹‰
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/                   # æ•°æ®é›†
â”‚   â””â”€â”€ 306/
â”œâ”€â”€ doc/                    # æ–‡æ¡£
â”‚   â”œâ”€â”€ installation.md
â”‚   â”œâ”€â”€ download.md
â”‚   â””â”€â”€ experiment_steps.md
â”œâ”€â”€ flame_model/            # FLAME æ¨¡å‹
â”œâ”€â”€ gaussian_renderer/      # é«˜æ–¯æ¸²æŸ“å™¨
â”œâ”€â”€ mesh_renderer/          # ç½‘æ ¼æ¸²æŸ“å™¨
â”œâ”€â”€ output/                 # è®­ç»ƒè¾“å‡º
â”‚   â”œâ”€â”€ exp1_baseline_306/
â”‚   â”œâ”€â”€ exp2_perceptual_306/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ scene/                  # åœºæ™¯ç®¡ç†
â”‚   â”œâ”€â”€ gaussian_model.py
â”‚   â””â”€â”€ flame_gaussian_model.py
â”œâ”€â”€ utils/                  # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ perceptual_loss.py
â”‚   â”œâ”€â”€ adaptive_densification.py
â”‚   â””â”€â”€ temporal_consistency.py
â”œâ”€â”€ train.py                # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ render.py               # æ¸²æŸ“è„šæœ¬
â”œâ”€â”€ metrics.py              # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ local_viewer.py         # æœ¬åœ°æŸ¥çœ‹å™¨
â”œâ”€â”€ remote_viewer.py        # è¿œç¨‹æŸ¥çœ‹å™¨
â”œâ”€â”€ requirements.txt        # ä¾èµ–åˆ—è¡¨
â””â”€â”€ EXPERIMENT_GUIDE.md     # æœ¬æ–‡æ¡£
```

---

**å®éªŒå®Œæˆæ ‡å¿—**:
- âœ… 8 ä¸ªå®éªŒå…¨éƒ¨è®­ç»ƒå®Œæˆ
- âœ… æ‰€æœ‰å®éªŒæ¸²æŸ“å®Œæˆ
- âœ… è¯„ä¼°æŒ‡æ ‡è®¡ç®—å®Œæˆ
- âœ… ç»“æœæ±‡æ€»ä¸å¯è§†åŒ–å®Œæˆ
- âœ… FPS åŸºå‡†æµ‹è¯•å®Œæˆ

**é¢„æœŸæˆæœ**:
1. å®šé‡è¯æ˜ä¸‰ä¸ªåˆ›æ–°æ¨¡å—çš„æœ‰æ•ˆæ€§
2. å®šæ€§å±•ç¤ºæ¸²æŸ“è´¨é‡çš„æå‡
3. åˆ†æåˆ›æ–°ä¹‹é—´çš„ååŒæ•ˆåº”
4. ä¸ºæœªæ¥ç ”ç©¶æä¾›åŸºå‡†å’Œè§è§£

ç¥å®éªŒé¡ºåˆ©ï¼ğŸš€
