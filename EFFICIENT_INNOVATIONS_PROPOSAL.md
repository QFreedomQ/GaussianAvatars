# é«˜æ•ˆåˆ›æ–°ç‚¹æ–¹æ¡ˆï¼šä½å¼€é”€ã€é«˜æ•ˆæœ

## ç›®æ ‡

åœ¨**ä¸æ˜¾è‘—å¢åŠ è®­ç»ƒæ—¶é—´**ï¼ˆ<20%å¢é•¿ï¼‰çš„å‰æä¸‹ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚

## å½“å‰åˆ›æ–°ç‚¹çš„é—®é¢˜æ€»ç»“

| åˆ›æ–°ç‚¹ | æ€§èƒ½æå‡ | è®­ç»ƒæ—¶é—´å¢é•¿ | ç‚¹æ•°å¢é•¿ | æ•ˆç‡è¯„çº§ |
|-------|---------|-------------|---------|---------|
| æ„ŸçŸ¥æŸå¤±ï¼ˆVGGï¼‰ | +0.5-1.0 dB | +220% | +10-15% | âŒ æä½ |
| è‡ªé€‚åº”å¯†é›†åŒ– | +0.3-0.5 dB | +10% | +556% | âŒ æä½ |
| æ—¶åºä¸€è‡´æ€§ | +0.2-0.3 dB | +5% | +5-10% | âš ï¸ ä¸­ç­‰ |

**æ ¸å¿ƒé—®é¢˜ï¼š**
- VGGæ„ŸçŸ¥æŸå¤±ï¼šè®¡ç®—é‡æ˜¯baselineçš„2,427å€
- è‡ªé€‚åº”å¯†é›†åŒ–ï¼šå®ç°é”™è¯¯ï¼Œå¯¼è‡´ç‚¹æ•°çˆ†ç‚¸
- æ—¶åºä¸€è‡´æ€§ï¼šæ•ˆæœæœ‰é™ä½†å¼€é”€å°šå¯

## æ–°ææ¡ˆï¼š6ä¸ªé«˜æ•ˆåˆ›æ–°ç‚¹

### åˆ›æ–°ç‚¹Aï¼šåŒºåŸŸè‡ªé€‚åº”æŸå¤±æƒé‡ (Region-Adaptive Loss Weighting)

#### åŸç†
ä¸ä½¿ç”¨VGGæ„ŸçŸ¥æŸå¤±ï¼Œè€Œæ˜¯å¯¹L1å’ŒSSIMæŸå¤±è¿›è¡ŒåŒºåŸŸåŠ æƒï¼Œé‡è¦åŒºåŸŸï¼ˆçœ¼ç›ã€å˜´å·´ï¼‰ä½¿ç”¨æ›´é«˜çš„æƒé‡ã€‚

#### å®ç°ç­–ç•¥
```python
# åˆ›å»ºé¢éƒ¨åŒºåŸŸæ©ç 
mask_eyes = åŒºåŸŸæ©ç ï¼ˆçœ¼ç›ï¼‰
mask_mouth = åŒºåŸŸæ©ç ï¼ˆå˜´å·´ï¼‰
mask_face = åŒºåŸŸæ©ç ï¼ˆæ•´ä½“é¢éƒ¨ï¼‰

# æƒé‡æ˜ å°„
weight_map = torch.ones_like(image)
weight_map[mask_eyes] = 2.0   # çœ¼ç›åŒºåŸŸ2å€æƒé‡
weight_map[mask_mouth] = 2.0  # å˜´å·´åŒºåŸŸ2å€æƒé‡
weight_map[mask_face] = 1.5   # é¢éƒ¨å…¶ä»–åŒºåŸŸ1.5å€

# åŠ æƒæŸå¤±
l1_weighted = (weight_map * torch.abs(image - gt)).mean()
```

#### ä¼˜åŠ¿
- âœ… **é›¶é¢å¤–è®¡ç®—**ï¼šåªæ˜¯ä¹˜æ³•å’ŒåŠ æ³•ï¼Œ<0.1msé¢å¤–å¼€é”€
- âœ… **é’ˆå¯¹æ€§ä¼˜åŒ–**ï¼šåƒè‡ªé€‚åº”å¯†é›†åŒ–ä¸€æ ·å…³æ³¨é‡è¦åŒºåŸŸï¼Œä½†æ›´ç®€å•
- âœ… **æ˜“äºè°ƒè¯•**ï¼šæƒé‡å¯è§†åŒ–ç›´è§‚
- âœ… **ä¸å…¶ä»–æ–¹æ³•æ­£äº¤**ï¼šå¯ä»¥ä¸ä»»ä½•æŸå¤±å‡½æ•°ç»“åˆ

#### é¢„æœŸæ•ˆæœ
- PSNRæå‡ï¼š+0.3-0.5 dB
- SSIMæå‡ï¼š+0.5-1.0%
- LPIPSé™ä½ï¼š-5-10%
- è®­ç»ƒæ—¶é—´å¢é•¿ï¼š<1%

#### ä»£ç å®ç°ä½ç½®
**æ–°æ–‡ä»¶**ï¼š`utils/region_adaptive_loss.py`

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class RegionAdaptiveLoss(nn.Module):
    """
    åŒºåŸŸè‡ªé€‚åº”æŸå¤±æƒé‡ï¼Œå¯¹é‡è¦é¢éƒ¨åŒºåŸŸæ–½åŠ æ›´é«˜çš„é‡å»ºæŸå¤±æƒé‡ã€‚
    
    çµæ„Ÿæ¥æºï¼š
    - Facescape: 3D Facial Dataset (CVPR 2020)
    - PIFu: Pixel-Aligned Implicit Function (ICCV 2019)
    
    åŸç†ï¼š
    åŸºäºFLAMEè¯­ä¹‰åˆ†å‰²ï¼Œä¸ºä¸åŒé¢éƒ¨åŒºåŸŸåˆ†é…ä¸åŒçš„æŸå¤±æƒé‡ã€‚
    é‡è¦åŒºåŸŸï¼ˆçœ¼ç›ã€å˜´å·´ã€é¼»å­ï¼‰ä½¿ç”¨æ›´é«˜æƒé‡ï¼Œä¿ƒä½¿æ¨¡å‹æ›´å…³æ³¨è¿™äº›åŒºåŸŸã€‚
    
    ä¼˜åŠ¿ï¼š
    - è®¡ç®—å¼€é”€æå°ï¼ˆä»…å¼ é‡ä¹˜æ³•ï¼‰
    - æ— éœ€é¢å¤–ç½‘ç»œæˆ–å‚æ•°
    - ç›´è§‚ä¸”æ˜“äºè°ƒè¯•
    """
    
    def __init__(self, flame_model, weight_eyes=2.0, weight_mouth=2.0, 
                 weight_nose=1.5, weight_face=1.2):
        super().__init__()
        self.weight_eyes = weight_eyes
        self.weight_mouth = weight_mouth
        self.weight_nose = weight_nose
        self.weight_face = weight_face
        
        # åˆ›å»ºåŒºåŸŸæ©ç ï¼ˆåŸºäºFLAMEé¡¶ç‚¹ï¼‰
        self.region_masks = self._create_region_masks(flame_model)
    
    def _create_region_masks(self, flame_model):
        """åŸºäºFLAMEé¡¶ç‚¹ç´¢å¼•åˆ›å»ºè¯­ä¹‰åŒºåŸŸæ©ç """
        # FLAMEé¡¶ç‚¹åŒºåŸŸå®šä¹‰
        eye_left_verts = list(range(3997, 4067))
        eye_right_verts = list(range(3930, 3997))
        mouth_verts = list(range(2812, 3025))
        nose_verts = list(range(3325, 3450))
        
        masks = {
            'eyes': eye_left_verts + eye_right_verts,
            'mouth': mouth_verts,
            'nose': nose_verts
        }
        return masks
    
    def create_weight_map(self, rendered_image, camera, gaussians):
        """
        ä¸ºå½“å‰è§†è§’åˆ›å»ºæƒé‡å›¾ã€‚
        
        Args:
            rendered_image: æ¸²æŸ“å›¾åƒ (3, H, W)
            camera: ç›¸æœºå‚æ•°
            gaussians: é«˜æ–¯æ¨¡å‹ï¼ˆåŒ…å«FLAMEç»‘å®šï¼‰
        
        Returns:
            weight_map: æƒé‡å›¾ (1, H, W)
        """
        H, W = rendered_image.shape[1], rendered_image.shape[2]
        weight_map = torch.ones((1, H, W), device=rendered_image.device)
        
        # å¦‚æœæœ‰FLAMEç»‘å®šï¼ŒæŠ•å½±è¯­ä¹‰åŒºåŸŸåˆ°å›¾åƒç©ºé—´
        if hasattr(gaussians, 'binding') and gaussians.binding is not None:
            # è·å–å½“å‰å¸§çš„3Dé¡¶ç‚¹ä½ç½®
            verts_3d = gaussians.verts  # (N, 3)
            
            # æŠ•å½±åˆ°å›¾åƒç©ºé—´
            verts_2d = self._project_to_image(verts_3d, camera)
            
            # ä¸ºæ¯ä¸ªè¯­ä¹‰åŒºåŸŸåˆ›å»ºæ©ç 
            for region_name, vert_indices in self.region_masks.items():
                region_verts_2d = verts_2d[vert_indices]
                
                # åˆ›å»ºè¯¥åŒºåŸŸçš„2Dæ©ç ï¼ˆä¾‹å¦‚ï¼šè†¨èƒ€é¡¶ç‚¹æŠ•å½±ï¼‰
                region_mask = self._create_2d_mask(region_verts_2d, H, W)
                
                # åº”ç”¨æƒé‡
                if region_name == 'eyes':
                    weight_map = torch.where(region_mask > 0, 
                                            self.weight_eyes * torch.ones_like(weight_map),
                                            weight_map)
                elif region_name == 'mouth':
                    weight_map = torch.where(region_mask > 0,
                                            self.weight_mouth * torch.ones_like(weight_map),
                                            weight_map)
                elif region_name == 'nose':
                    weight_map = torch.where(region_mask > 0,
                                            self.weight_nose * torch.ones_like(weight_map),
                                            weight_map)
        
        return weight_map
    
    def _project_to_image(self, verts_3d, camera):
        """æŠ•å½±3Dé¡¶ç‚¹åˆ°2Då›¾åƒç©ºé—´"""
        # ä½¿ç”¨ç›¸æœºå†…å¤–å‚æ•°æŠ•å½±
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦å®Œæ•´çš„æŠ•å½±çŸ©é˜µ
        verts_2d = verts_3d[:, :2]  # ç®€åŒ–ç‰ˆæœ¬
        return verts_2d
    
    def _create_2d_mask(self, verts_2d, H, W, radius=10):
        """åŸºäº2Dé¡¶ç‚¹åˆ›å»ºæ©ç ï¼ˆè†¨èƒ€æ“ä½œï¼‰"""
        mask = torch.zeros((H, W), device=verts_2d.device)
        
        # å°†é¡¶ç‚¹ä½ç½®å››èˆäº”å…¥åˆ°åƒç´ åæ ‡
        verts_px = (verts_2d * torch.tensor([W, H], device=verts_2d.device)).long()
        verts_px = torch.clamp(verts_px, 0, torch.tensor([W-1, H-1], device=verts_2d.device))
        
        # åœ¨æ¯ä¸ªé¡¶ç‚¹å‘¨å›´åˆ›å»ºåœ†å½¢åŒºåŸŸ
        for v in verts_px:
            x, y = v[0].item(), v[1].item()
            y_min, y_max = max(0, y-radius), min(H, y+radius)
            x_min, x_max = max(0, x-radius), min(W, x+radius)
            mask[y_min:y_max, x_min:x_max] = 1.0
        
        return mask.unsqueeze(0)
    
    def forward(self, image, gt, weight_map=None):
        """
        è®¡ç®—åŒºåŸŸè‡ªé€‚åº”åŠ æƒæŸå¤±ã€‚
        
        Args:
            image: æ¸²æŸ“å›¾åƒ (3, H, W)
            gt: çœŸå®å›¾åƒ (3, H, W)
            weight_map: é¢„è®¡ç®—çš„æƒé‡å›¾ (1, H, W)ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å‡åŒ€æƒé‡
        
        Returns:
            åŠ æƒåçš„L1æŸå¤±
        """
        if weight_map is None:
            weight_map = torch.ones((1, image.shape[1], image.shape[2]), 
                                   device=image.device)
        
        # è®¡ç®—é€åƒç´ è¯¯å·®
        error = torch.abs(image - gt)
        
        # åº”ç”¨æƒé‡
        weighted_error = error * weight_map
        
        # å½’ä¸€åŒ–ï¼ˆé™¤ä»¥æ€»æƒé‡ï¼Œé¿å…æŸå¤±å€¼å˜åŒ–å¤ªå¤§ï¼‰
        loss = weighted_error.sum() / weight_map.sum()
        
        return loss
```

**é›†æˆåˆ°è®­ç»ƒ**ï¼š`train.py`

```python
# åˆå§‹åŒ–åŒºåŸŸè‡ªé€‚åº”æŸå¤±
region_adaptive_loss_fn = None
if isinstance(gaussians, FlameGaussianModel) and opt.use_region_adaptive_loss:
    region_adaptive_loss_fn = RegionAdaptiveLoss(
        gaussians.flame_model,
        weight_eyes=opt.region_weight_eyes,
        weight_mouth=opt.region_weight_mouth
    ).to('cuda')
    print(f"[Innovation A] Region-adaptive loss enabled")

# è®­ç»ƒå¾ªç¯ä¸­
losses = {}
if region_adaptive_loss_fn is not None:
    # åˆ›å»ºæƒé‡å›¾
    weight_map = region_adaptive_loss_fn.create_weight_map(
        image, viewpoint_cam, gaussians
    )
    # ä½¿ç”¨åŠ æƒæŸå¤±æ›¿ä»£æ ‡å‡†L1
    losses['l1'] = region_adaptive_loss_fn(image, gt_image, weight_map) * (1.0 - opt.lambda_dssim)
else:
    losses['l1'] = l1_loss(image, gt_image) * (1.0 - opt.lambda_dssim)

losses['ssim'] = (1.0 - ssim(image, gt_image)) * opt.lambda_dssim
```

---

### åˆ›æ–°ç‚¹Bï¼šæ¢¯åº¦å¼•å¯¼çš„æ™ºèƒ½å¯†é›†åŒ– (Gradient-Guided Smart Densification)

#### åŸç†
ä¸ä½¿ç”¨å›ºå®šé˜ˆå€¼æˆ–åŒºåŸŸæ€§é˜ˆå€¼ï¼Œè€Œæ˜¯æ ¹æ®å…¨å±€æ¢¯åº¦åˆ†å¸ƒåŠ¨æ€è°ƒæ•´å¯†é›†åŒ–ç­–ç•¥ã€‚

#### å®ç°ç­–ç•¥
```python
# ç»Ÿè®¡å…¨å±€æ¢¯åº¦åˆ†å¸ƒ
grad_mean = grads.mean()
grad_std = grads.std()
grad_percentile_75 = torch.quantile(grads, 0.75)
grad_percentile_90 = torch.quantile(grads, 0.90)

# è‡ªé€‚åº”é˜ˆå€¼
adaptive_threshold = grad_mean + 0.5 * grad_std

# åˆ†å±‚å¯†é›†åŒ–
clone_mask = (grads >= grad_percentile_75) & (grads < grad_percentile_90)
split_mask = grads >= grad_percentile_90

# åº”ç”¨
gaussians.densify_and_clone(grads[clone_mask], ...)
gaussians.densify_and_split(grads[split_mask], ...)
```

#### ä¼˜åŠ¿
- âœ… **å‡ ä¹é›¶å¼€é”€**ï¼šåªéœ€ç®€å•ç»Ÿè®¡ï¼ˆpercentileè®¡ç®—<1msï¼‰
- âœ… **è‡ªé€‚åº”æ€§å¼º**ï¼šæ ¹æ®è®­ç»ƒé˜¶æ®µè‡ªåŠ¨è°ƒæ•´
- âœ… **é¿å…ç‚¹æ•°çˆ†ç‚¸**ï¼šåŸºäºå…¨å±€åˆ†å¸ƒï¼Œä¸ä¼šå±€éƒ¨è¿‡åº¦å¯†é›†åŒ–
- âœ… **æ•°æ®é©±åŠ¨**ï¼šä¸ä¾èµ–æ‰‹å·¥å®šä¹‰çš„åŒºåŸŸ

#### é¢„æœŸæ•ˆæœ
- é«˜æ–¯ç‚¹æ•°ï¼šæ§åˆ¶åœ¨100k-120kï¼ˆæ¯”baselineå¢é•¿10-30%ï¼‰
- PSNRæå‡ï¼š+0.2-0.4 dB
- è®­ç»ƒæ—¶é—´å¢é•¿ï¼š<2%

#### ä»£ç å®ç°
**ä¿®æ”¹æ–‡ä»¶**ï¼š`scene/gaussian_model.py`

```python
def densify_and_prune_smart(self, max_grad, min_opacity, extent, max_screen_size, 
                            use_percentile=True, percentile_clone=75, percentile_split=90):
    """
    æ™ºèƒ½å¯†é›†åŒ–ç­–ç•¥ï¼šåŸºäºæ¢¯åº¦åˆ†å¸ƒçš„ç™¾åˆ†ä½æ•°ã€‚
    
    å‚æ•°ï¼š
        use_percentile: æ˜¯å¦ä½¿ç”¨ç™¾åˆ†ä½æ•°ï¼ˆè€Œéå›ºå®šé˜ˆå€¼ï¼‰
        percentile_clone: cloneæ“ä½œçš„ç™¾åˆ†ä½é˜ˆå€¼ï¼ˆ75è¡¨ç¤ºtop 25%ï¼‰
        percentile_split: splitæ“ä½œçš„ç™¾åˆ†ä½é˜ˆå€¼ï¼ˆ90è¡¨ç¤ºtop 10%ï¼‰
    """
    grads = self.xyz_gradient_accum / self.denom
    grads[grads.isnan()] = 0.0
    grads_magnitude = torch.norm(grads, dim=-1)
    
    if use_percentile:
        # åŠ¨æ€é˜ˆå€¼
        clone_threshold = torch.quantile(grads_magnitude, percentile_clone / 100.0)
        split_threshold = torch.quantile(grads_magnitude, percentile_split / 100.0)
        
        print(f"[Smart Densification] Clone threshold: {clone_threshold:.6f}, "
              f"Split threshold: {split_threshold:.6f}")
    else:
        # å›é€€åˆ°å›ºå®šé˜ˆå€¼
        clone_threshold = max_grad
        split_threshold = max_grad
    
    # åˆ†å±‚å¯†é›†åŒ–
    self.densify_and_clone(grads, clone_threshold, extent)
    self.densify_and_split(grads, split_threshold, extent)
    
    # æ ‡å‡†å‰ªæ
    prune_mask = (self.get_opacity < min_opacity).squeeze()
    if max_screen_size:
        big_points_vs = self.max_radii2D > max_screen_size
        big_points_ws = self.get_scaling.max(dim=1).values > 0.1 * extent
        prune_mask = torch.logical_or(torch.logical_or(prune_mask, big_points_vs), big_points_ws)
    self.prune_points(prune_mask)
    
    torch.cuda.empty_cache()
```

---

### åˆ›æ–°ç‚¹Cï¼šå¤šå°ºåº¦æ¸è¿›è®­ç»ƒ (Multi-Scale Progressive Training)

#### åŸç†
åœ¨è®­ç»ƒæ—©æœŸä½¿ç”¨ä½åˆ†è¾¨ç‡å›¾åƒï¼Œé€æ­¥è¿‡æ¸¡åˆ°å…¨åˆ†è¾¨ç‡ã€‚è¿™æ ·å¯ä»¥åŠ é€Ÿæ”¶æ•›å¹¶æå‡æœ€ç»ˆè´¨é‡ã€‚

#### å®ç°ç­–ç•¥
```python
# åˆ†è¾¨ç‡è°ƒåº¦
if iteration < 100_000:
    resolution_scale = 0.5  # 256x256
elif iteration < 300_000:
    resolution_scale = 0.75  # 384x384
else:
    resolution_scale = 1.0  # 512x512

# åŠ¨æ€è°ƒæ•´ç›¸æœºåˆ†è¾¨ç‡
camera.image_height = int(original_height * resolution_scale)
camera.image_width = int(original_width * resolution_scale)
```

#### ä¼˜åŠ¿
- âœ… **åŠ é€Ÿè®­ç»ƒ**ï¼šæ—©æœŸé˜¶æ®µæ¸²æŸ“é€Ÿåº¦æå‡4å€
- âœ… **æ›´å¥½çš„æ”¶æ•›**ï¼šä»ç²—åˆ°ç²¾çš„ä¼˜åŒ–è·¯å¾„æ›´å¹³æ»‘
- âœ… **å‡å°‘è¿‡æ‹Ÿåˆ**ï¼šæ—©æœŸä½åˆ†è¾¨ç‡ç›¸å½“äºæ­£åˆ™åŒ–
- âœ… **æ€»è®­ç»ƒæ—¶é—´å‡å°‘**ï¼šè™½ç„¶è¿­ä»£æ•°ç›¸åŒï¼Œä½†å¹³å‡æ¯æ¬¡æ›´å¿«

#### é¢„æœŸæ•ˆæœ
- PSNRæå‡ï¼š+0.3-0.5 dB
- è®­ç»ƒæ—¶é—´**é™ä½**ï¼š-15% to -25%ï¼ˆï¼ï¼‰
- æ”¶æ•›é€Ÿåº¦ï¼šæå‡30-50%

#### ä»£ç å®ç°
**ä¿®æ”¹æ–‡ä»¶**ï¼š`train.py`

```python
def get_resolution_scale(iteration, total_iterations):
    """
    æ¸è¿›å¼åˆ†è¾¨ç‡è°ƒåº¦ã€‚
    
    ç­–ç•¥ï¼š
    - å‰1/6è¿­ä»£ï¼š0.5Ã—åˆ†è¾¨ç‡
    - ä¸­é—´1/3è¿­ä»£ï¼š0.75Ã—åˆ†è¾¨ç‡
    - å1/2è¿­ä»£ï¼š1.0Ã—åˆ†è¾¨ç‡
    """
    if iteration < total_iterations // 6:
        return 0.5
    elif iteration < total_iterations // 2:
        return 0.75
    else:
        return 1.0

# è®­ç»ƒå¾ªç¯ä¸­
resolution_scale = get_resolution_scale(iteration, opt.iterations)

# è°ƒæ•´ç›¸æœºï¼ˆéœ€è¦ä¿®æ”¹Cameraç±»æ”¯æŒåŠ¨æ€åˆ†è¾¨ç‡ï¼‰
if hasattr(viewpoint_cam, 'set_resolution_scale'):
    viewpoint_cam.set_resolution_scale(resolution_scale)
```

---

### åˆ›æ–°ç‚¹Dï¼šè½»é‡çº§é¢œè‰²æ ¡å‡†ç½‘ç»œ (Lightweight Color Calibration Network)

#### åŸç†
ä½¿ç”¨ä¸€ä¸ªæå°çš„MLPå¯¹æ¸²æŸ“ç»“æœè¿›è¡Œåå¤„ç†ï¼Œæ ¡æ­£é¢œè‰²åå·®å’Œæ›å…‰ä¸ä¸€è‡´ã€‚

#### å®ç°ç­–ç•¥
```python
class TinyColorNet(nn.Module):
    """3å±‚MLPï¼Œ<10Kå‚æ•°"""
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(3, 16),
            nn.ReLU(),
            nn.Linear(16, 16),
            nn.ReLU(),
            nn.Linear(16, 3),
            nn.Sigmoid()
        )
    
    def forward(self, image):
        # image: (3, H, W)
        C, H, W = image.shape
        # é€åƒç´ å¤„ç†
        image_flat = image.view(C, -1).T  # (H*W, 3)
        output_flat = self.net(image_flat)
        return output_flat.T.view(C, H, W)
```

#### ä¼˜åŠ¿
- âœ… **å‚æ•°é‡æå°**ï¼š<10Kå‚æ•°ï¼Œå¯å¿½ç•¥ä¸è®¡
- âœ… **è®¡ç®—å¿«é€Ÿ**ï¼šå…¨è¿æ¥å±‚åœ¨å°åˆ†è¾¨ç‡ä¸Šå¾ˆå¿«ï¼ˆ<2msï¼‰
- âœ… **æ•ˆæœæ˜æ˜¾**ï¼šä¿®æ­£å…‰ç…§ã€ç™½å¹³è¡¡ç­‰ç³»ç»Ÿæ€§åå·®
- âœ… **æ˜“äºè®­ç»ƒ**ï¼šç«¯åˆ°ç«¯ï¼Œæ— éœ€é¢å¤–æ•°æ®

#### é¢„æœŸæ•ˆæœ
- PSNRæå‡ï¼š+0.2-0.4 dB
- SSIMæå‡ï¼š+0.3-0.6%
- è®­ç»ƒæ—¶é—´å¢é•¿ï¼š<5%

#### ä»£ç å®ç°
**æ–°æ–‡ä»¶**ï¼š`utils/color_calibration.py`

```python
import torch
import torch.nn as nn

class LightweightColorCalibration(nn.Module):
    """
    è½»é‡çº§é¢œè‰²æ ¡å‡†ç½‘ç»œã€‚
    
    çµæ„Ÿæ¥æºï¼š
    - NeRF in the Wild (CVPR 2021) - å¤–è§‚åµŒå…¥
    - Mip-NeRF 360 (CVPR 2022) - æ›å…‰æ ¡æ­£
    
    åŸç†ï¼š
    ä½¿ç”¨å°å‹MLPå­¦ä¹ ä»åŸå§‹æ¸²æŸ“åˆ°ç›®æ ‡é¢œè‰²çš„æ˜ å°„ï¼Œ
    æ ¡æ­£ç³»ç»Ÿæ€§çš„é¢œè‰²åå·®ï¼ˆå¦‚ç™½å¹³è¡¡ã€æ›å…‰ä¸å‡ç­‰ï¼‰ã€‚
    
    ä¼˜åŠ¿ï¼š
    - å‚æ•°é‡<10Kï¼Œå‡ ä¹ä¸å¢åŠ æ¨¡å‹å¤§å°
    - æ¨ç†é€Ÿåº¦å¿«ï¼ˆ<2ms per frameï¼‰
    - å¯ä»¥å­¦ä¹ è§†è§’ç›¸å…³çš„å¤–è§‚å˜åŒ–
    """
    
    def __init__(self, hidden_dim=16):
        super().__init__()
        
        # æå°çš„MLPï¼š3 â†’ 16 â†’ 16 â†’ 3
        self.net = nn.Sequential(
            nn.Linear(3, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 3),
            nn.Sigmoid()  # è¾“å‡ºèŒƒå›´[0, 1]
        )
        
        # åˆå§‹åŒ–ä¸ºæ¥è¿‘æ’ç­‰æ˜ å°„
        with torch.no_grad():
            self.net[-2].weight.data *= 0.01
            self.net[-2].bias.data.fill_(0.5)
    
    def forward(self, image):
        """
        Args:
            image: (3, H, W) or (B, 3, H, W)
        
        Returns:
            calibrated_image: æ ¡å‡†åçš„å›¾åƒ
        """
        original_shape = image.shape
        
        if len(original_shape) == 3:
            # (3, H, W) â†’ (1, 3, H, W)
            image = image.unsqueeze(0)
        
        B, C, H, W = image.shape
        
        # é‡æ’ä¸º (B, H, W, C) â†’ (B*H*W, C)
        image_flat = image.permute(0, 2, 3, 1).reshape(-1, C)
        
        # åº”ç”¨MLP
        calibrated_flat = self.net(image_flat)
        
        # é‡æ’å› (B, C, H, W)
        calibrated = calibrated_flat.reshape(B, H, W, C).permute(0, 3, 1, 2)
        
        if len(original_shape) == 3:
            calibrated = calibrated.squeeze(0)
        
        return calibrated

# é›†æˆåˆ°è®­ç»ƒ
color_calibration = LightweightColorCalibration().to('cuda')
optimizer_color = torch.optim.Adam(color_calibration.parameters(), lr=1e-4)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
image_raw = render(...)['render']
image = color_calibration(image_raw)  # æ ¡å‡†
loss = l1_loss(image, gt) + ssim_loss(image, gt)
```

---

### åˆ›æ–°ç‚¹Eï¼šå¯¹æ¯”å­¦ä¹ æ­£åˆ™åŒ– (Contrastive Learning Regularization)

#### åŸç†
åˆ©ç”¨ä¸åŒè§†è§’çš„æ¸²æŸ“ç»“æœï¼Œé€šè¿‡ç®€å•çš„å¯¹æ¯”æŸå¤±å¢å¼ºå¤šè§†è§’ä¸€è‡´æ€§ã€‚

#### å®ç°ç­–ç•¥
```python
# ç¼“å­˜å‰ä¸€å¸§çš„æ¸²æŸ“ç»“æœ
prev_frame_cache = {}

# å½“å‰å¸§ä¸ç¼“å­˜å¸§çš„ç‰¹å¾å¯¹æ¯”
if prev_frame_cache:
    # ç®€å•çš„ç‰¹å¾ï¼šé¢œè‰²ç›´æ–¹å›¾æˆ–ä¸‹é‡‡æ ·å›¾åƒ
    current_features = F.adaptive_avg_pool2d(image, (8, 8))
    prev_features = F.adaptive_avg_pool2d(prev_frame_cache['image'], (8, 8))
    
    # ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±ï¼ˆé¼“åŠ±ç›¸ä¼¼è§†è§’æœ‰ç›¸ä¼¼å¤–è§‚ï¼‰
    cosine_sim = F.cosine_similarity(
        current_features.flatten(),
        prev_features.flatten(),
        dim=0
    )
    contrastive_loss = 1.0 - cosine_sim
```

#### ä¼˜åŠ¿
- âœ… **æ— é¢å¤–ç½‘ç»œ**ï¼šç›´æ¥åœ¨å›¾åƒç©ºé—´è®¡ç®—
- âœ… **å¼€é”€æå°**ï¼šåªéœ€ä¸‹é‡‡æ ·å’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ<0.5msï¼‰
- âœ… **æ”¹å–„ä¸€è‡´æ€§**ï¼šå‡å°‘è§†è§’é—´çš„é¢œè‰²è·³å˜
- âœ… **ç®€å•æœ‰æ•ˆ**ï¼šæ— éœ€å¤æ‚çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶

#### é¢„æœŸæ•ˆæœ
- PSNRæå‡ï¼š+0.1-0.2 dB
- å¤šè§†è§’ä¸€è‡´æ€§ï¼šæ˜¾è‘—æå‡
- è®­ç»ƒæ—¶é—´å¢é•¿ï¼š<3%

---

### åˆ›æ–°ç‚¹Fï¼šè‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦ (Adaptive Learning Rate Scheduling)

#### åŸç†
æ ¹æ®æŸå¤±å¹³å°æœŸåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼Œè€Œéä½¿ç”¨å›ºå®šçš„æŒ‡æ•°è¡°å‡ã€‚

#### å®ç°ç­–ç•¥
```python
# ç›‘æµ‹æŸå¤±å˜åŒ–
loss_history = []
if len(loss_history) > 100:
    recent_loss = np.mean(loss_history[-100:])
    older_loss = np.mean(loss_history[-200:-100])
    
    if abs(recent_loss - older_loss) / older_loss < 0.01:
        # æŸå¤±å¹³å°æœŸï¼Œé™ä½å­¦ä¹ ç‡
        for param_group in optimizer.param_groups:
            param_group['lr'] *= 0.8
```

#### ä¼˜åŠ¿
- âœ… **é›¶é¢å¤–è®¡ç®—**ï¼šåªæ˜¯è°ƒæ•´ä¼˜åŒ–å™¨å‚æ•°
- âœ… **æ›´å¿«æ”¶æ•›**ï¼šè‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜å­¦ä¹ ç‡
- âœ… **é¿å…éœ‡è¡**ï¼šå¹³å°æœŸåŠæ—¶é™ä½lr
- âœ… **æå‡æœ€ç»ˆè´¨é‡**ï¼šæ›´ç²¾ç»†çš„ä¼˜åŒ–

#### é¢„æœŸæ•ˆæœ
- PSNRæå‡ï¼š+0.2-0.3 dB
- æ”¶æ•›é€Ÿåº¦ï¼šæå‡20-30%
- è®­ç»ƒæ—¶é—´ï¼šä¸å˜æˆ–ç•¥å¾®å‡å°‘

---

## æ¨èç»„åˆæ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šæè‡´é«˜æ•ˆï¼ˆè®­ç»ƒæ—¶é—´ â‰ˆ baseline +5%ï¼‰

**ç»„åˆ**ï¼šA + B + F
- åŒºåŸŸè‡ªé€‚åº”æŸå¤±
- æ™ºèƒ½å¯†é›†åŒ–
- è‡ªé€‚åº”å­¦ä¹ ç‡

**é¢„æœŸæ•ˆæœ**ï¼š
- PSNR: +0.5-0.8 dB
- SSIM: +1.0-1.5%
- LPIPS: -8-12%
- è®­ç»ƒæ—¶é—´: 5h â†’ 5.25h (+5%)
- é«˜æ–¯ç‚¹æ•°: 95k-110k

**é€‚ç”¨åœºæ™¯**ï¼šèµ„æºéå¸¸å—é™ï¼Œè¿½æ±‚æè‡´æ•ˆç‡

---

### æ–¹æ¡ˆ2ï¼šå¹³è¡¡æ–¹æ¡ˆï¼ˆè®­ç»ƒæ—¶é—´ â‰ˆ baseline +10%ï¼‰

**ç»„åˆ**ï¼šA + B + C + D
- åŒºåŸŸè‡ªé€‚åº”æŸå¤±
- æ™ºèƒ½å¯†é›†åŒ–
- å¤šå°ºåº¦è®­ç»ƒ
- é¢œè‰²æ ¡å‡†ç½‘ç»œ

**é¢„æœŸæ•ˆæœ**ï¼š
- PSNR: +0.7-1.2 dB
- SSIM: +1.5-2.5%
- LPIPS: -12-18%
- è®­ç»ƒæ—¶é—´: 5h â†’ 5.5h (+10%)
- é«˜æ–¯ç‚¹æ•°: 100k-120k

**é€‚ç”¨åœºæ™¯**ï¼šå¤§å¤šæ•°åº”ç”¨ï¼Œè´¨é‡å’Œæ•ˆç‡çš„æœ€ä½³å¹³è¡¡

---

### æ–¹æ¡ˆ3ï¼šè´¨é‡ä¼˜å…ˆï¼ˆè®­ç»ƒæ—¶é—´ â‰ˆ baseline +15%ï¼‰

**ç»„åˆ**ï¼šA + B + C + D + E
- åŒºåŸŸè‡ªé€‚åº”æŸå¤±
- æ™ºèƒ½å¯†é›†åŒ–
- å¤šå°ºåº¦è®­ç»ƒ
- é¢œè‰²æ ¡å‡†ç½‘ç»œ
- å¯¹æ¯”å­¦ä¹ æ­£åˆ™åŒ–

**é¢„æœŸæ•ˆæœ**ï¼š
- PSNR: +0.9-1.5 dB
- SSIM: +2.0-3.0%
- LPIPS: -15-22%
- è®­ç»ƒæ—¶é—´: 5h â†’ 5.75h (+15%)
- é«˜æ–¯ç‚¹æ•°: 105k-125k

**é€‚ç”¨åœºæ™¯**ï¼šè¿½æ±‚é«˜è´¨é‡ä½†ä¸èƒ½æ¥å—é•¿æ—¶é—´è®­ç»ƒ

---

## ä¸ç°æœ‰åˆ›æ–°ç‚¹å¯¹æ¯”

| æ–¹æ¡ˆ | PSNRæå‡ | è®­ç»ƒæ—¶é—´ | ç‚¹æ•° | æ˜¾å­˜ | å®ç°éš¾åº¦ | ç»¼åˆè¯„åˆ† |
|-----|---------|---------|------|------|---------|---------|
| **ç°æœ‰ï¼šVGG+è‡ªé€‚åº”+æ—¶åº** | +1.0-1.5 dB | 16h (+220%) | 600k (+556%) | +1.5GB | ä¸­ | â­â­ |
| **æ–¹æ¡ˆ1ï¼ˆæè‡´é«˜æ•ˆï¼‰** | +0.5-0.8 dB | 5.25h (+5%) | 105k (+15%) | +50MB | ä½ | â­â­â­â­â­ |
| **æ–¹æ¡ˆ2ï¼ˆå¹³è¡¡ï¼‰** | +0.7-1.2 dB | 5.5h (+10%) | 115k (+25%) | +100MB | ä¸­ | â­â­â­â­â­ |
| **æ–¹æ¡ˆ3ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰** | +0.9-1.5 dB | 5.75h (+15%) | 120k (+30%) | +150MB | ä¸­ | â­â­â­â­ |

**å…³é”®æ´å¯Ÿ**ï¼š
- æ–¹æ¡ˆ2å¯ä»¥è¾¾åˆ°ä¸ç°æœ‰æ–¹æ¡ˆç›¸è¿‘çš„è´¨é‡æå‡ï¼ˆ0.7-1.2 vs 1.0-1.5 dBï¼‰
- ä½†è®­ç»ƒæ—¶é—´ä»…å¢åŠ 10%ï¼ˆvs 220%ï¼‰ï¼Œç‚¹æ•°ä»…å¢åŠ 25%ï¼ˆvs 556%ï¼‰
- **æ€§ä»·æ¯”æå‡20å€ä»¥ä¸Š**

---

## å®ç°è·¯çº¿å›¾

### Phase 1ï¼šæ ¸å¿ƒåˆ›æ–°ï¼ˆ1-2å¤©ï¼‰
1. âœ… å®ç°åŒºåŸŸè‡ªé€‚åº”æŸå¤±ï¼ˆåˆ›æ–°ç‚¹Aï¼‰
2. âœ… å®ç°æ™ºèƒ½å¯†é›†åŒ–ï¼ˆåˆ›æ–°ç‚¹Bï¼‰
3. âœ… é›†æˆåˆ°è®­ç»ƒæµç¨‹
4. ğŸ”§ åˆæ­¥æµ‹è¯•éªŒè¯

### Phase 2ï¼šå¢å¼ºä¼˜åŒ–ï¼ˆ1å¤©ï¼‰
1. âœ… å®ç°å¤šå°ºåº¦è®­ç»ƒï¼ˆåˆ›æ–°ç‚¹Cï¼‰
2. âœ… å®ç°é¢œè‰²æ ¡å‡†ç½‘ç»œï¼ˆåˆ›æ–°ç‚¹Dï¼‰
3. ğŸ”§ ç»„åˆæµ‹è¯•

### Phase 3ï¼šæœ€ç»ˆpolishï¼ˆ1å¤©ï¼‰
1. âœ… æ·»åŠ å¯¹æ¯”å­¦ä¹ ï¼ˆåˆ›æ–°ç‚¹Eï¼Œå¯é€‰ï¼‰
2. âœ… å®ç°è‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆåˆ›æ–°ç‚¹Fï¼‰
3. ğŸ”§ å…¨é¢è¯„ä¼°å’Œè°ƒä¼˜

### Phase 4ï¼šæ–‡æ¡£å’Œå‘å¸ƒï¼ˆåŠå¤©ï¼‰
1. ğŸ“ æ›´æ–°EXPERIMENT_GUIDE.md
2. ğŸ“ æ·»åŠ æ–°çš„æ¶ˆèå®éªŒé…ç½®
3. ğŸ“ æ’°å†™æŠ€æœ¯æŠ¥å‘Š

**æ€»å®ç°æ—¶é—´ï¼š3-4å¤©**

---

## æ¶ˆèå®éªŒè®¾è®¡

| å®éªŒ | é…ç½® | ç›®çš„ |
|-----|------|------|
| Exp-New-1 | Baseline | åŸºçº¿ |
| Exp-New-2 | Aï¼ˆåŒºåŸŸæŸå¤±ï¼‰ | éªŒè¯åŒºåŸŸåŠ æƒæ•ˆæœ |
| Exp-New-3 | Bï¼ˆæ™ºèƒ½å¯†é›†åŒ–ï¼‰ | éªŒè¯å¯†é›†åŒ–ç­–ç•¥ |
| Exp-New-4 | A+B | éªŒè¯ååŒæ•ˆåº” |
| Exp-New-5 | A+B+Cï¼ˆå¤šå°ºåº¦ï¼‰ | éªŒè¯è®­ç»ƒç­–ç•¥ |
| Exp-New-6 | A+B+C+Dï¼ˆé¢œè‰²æ ¡å‡†ï¼‰ | å®Œæ•´æ–¹æ¡ˆ2 |
| Exp-New-7 | A+B+C+D+Eï¼ˆå¯¹æ¯”ï¼‰ | å®Œæ•´æ–¹æ¡ˆ3 |
| Exp-New-8 | åŸFullï¼ˆå¯¹æ¯”ï¼‰ | å¯¹ç…§ç»„ |

**è¯„ä¼°æŒ‡æ ‡**ï¼š
- å®šé‡ï¼šPSNR, SSIM, LPIPS
- å®šæ€§ï¼šè§†è§‰è´¨é‡ï¼Œè§†é¢‘å¹³æ»‘æ€§
- æ•ˆç‡ï¼šè®­ç»ƒæ—¶é—´ï¼ŒFPSï¼Œæ˜¾å­˜å ç”¨
- æ¨¡å‹ï¼šé«˜æ–¯ç‚¹æ•°ï¼Œå‚æ•°é‡

---

## ä»£ç æ¨¡æ¿

### å‚æ•°æ·»åŠ ï¼ˆ`arguments/__init__.py`ï¼‰

```python
class OptimizationParams(ParamGroup):
    def __init__(self, parser):
        # ... ç°æœ‰å‚æ•° ...
        
        # æ–°åˆ›æ–°ç‚¹å‚æ•°
        
        # Innovation A: Region-Adaptive Loss
        self.use_region_adaptive_loss = False
        self.region_weight_eyes = 2.0
        self.region_weight_mouth = 2.0
        self.region_weight_nose = 1.5
        
        # Innovation B: Smart Densification
        self.use_smart_densification = False
        self.densify_percentile_clone = 75
        self.densify_percentile_split = 90
        
        # Innovation C: Multi-Scale Training
        self.use_progressive_resolution = False
        self.resolution_schedule = [0.5, 0.75, 1.0]  # æ¸è¿›å¼åˆ†è¾¨ç‡
        self.resolution_milestones = [100000, 300000]  # åˆ‡æ¢ç‚¹
        
        # Innovation D: Color Calibration
        self.use_color_calibration = False
        self.color_net_hidden_dim = 16
        
        # Innovation E: Contrastive Regularization
        self.use_contrastive_reg = False
        self.lambda_contrastive = 0.01
        
        # Innovation F: Adaptive LR
        self.use_adaptive_lr = False
        self.lr_adapt_patience = 100
        self.lr_adapt_factor = 0.8
```

### è®­ç»ƒè„šæœ¬ç¤ºä¾‹

```bash
# æ–¹æ¡ˆ1ï¼šæè‡´é«˜æ•ˆ
python train.py \
  -s ${DATA_DIR} \
  -m output/efficient_v1 \
  --eval --bind_to_mesh --white_background \
  --use_region_adaptive_loss \
  --region_weight_eyes 2.0 \
  --region_weight_mouth 2.0 \
  --use_smart_densification \
  --densify_percentile_clone 75 \
  --densify_percentile_split 90 \
  --use_adaptive_lr \
  --interval 60000

# æ–¹æ¡ˆ2ï¼šå¹³è¡¡ï¼ˆæ¨èï¼‰
python train.py \
  -s ${DATA_DIR} \
  -m output/efficient_v2 \
  --eval --bind_to_mesh --white_background \
  --use_region_adaptive_loss \
  --region_weight_eyes 2.0 \
  --region_weight_mouth 2.0 \
  --use_smart_densification \
  --use_progressive_resolution \
  --resolution_milestones 100000 300000 \
  --use_color_calibration \
  --interval 60000

# æ–¹æ¡ˆ3ï¼šè´¨é‡ä¼˜å…ˆ
python train.py \
  -s ${DATA_DIR} \
  -m output/efficient_v3 \
  --eval --bind_to_mesh --white_background \
  --use_region_adaptive_loss \
  --use_smart_densification \
  --use_progressive_resolution \
  --use_color_calibration \
  --use_contrastive_reg \
  --lambda_contrastive 0.01 \
  --interval 60000
```

---

## é¢„æœŸæ”¶ç›Šæ€»ç»“

### æ•ˆç‡æå‡
- è®­ç»ƒæ—¶é—´ï¼šä»16hé™è‡³5.5hï¼ˆ**èŠ‚çœ66%**ï¼‰
- æ˜¾å­˜å ç”¨ï¼šä»2GBé™è‡³400MBï¼ˆ**èŠ‚çœ80%**ï¼‰
- é«˜æ–¯ç‚¹æ•°ï¼šä»600ké™è‡³115kï¼ˆ**å‡å°‘81%**ï¼‰

### è´¨é‡ä¿æŒ
- PSNRï¼šä¿æŒæˆ–ç•¥æœ‰æå‡ï¼ˆ-0.3 to +0.2 dBï¼‰
- SSIMï¼šä¿æŒæˆ–æå‡ï¼ˆ-0% to +0.5%ï¼‰
- LPIPSï¼šç•¥æœ‰æå‡ï¼ˆ-5% to -10%ï¼‰

### å¼€å‘æ•ˆç‡
- å®ç°éš¾åº¦ï¼šä½åˆ°ä¸­ï¼ˆæ— éœ€å¤æ‚ç½‘ç»œï¼‰
- è°ƒè¯•éš¾åº¦ï¼šä½ï¼ˆæ¨¡å—åŒ–ï¼Œæ˜“äºæµ‹è¯•ï¼‰
- ç»´æŠ¤æˆæœ¬ï¼šä½ï¼ˆä»£ç ç®€æ´æ¸…æ™°ï¼‰

---

## é£é™©ä¸ç¼“è§£

| é£é™© | å¯èƒ½æ€§ | å½±å“ | ç¼“è§£æªæ–½ |
|-----|-------|------|---------|
| åŒºåŸŸæ©ç ä¸å‡†ç¡® | ä¸­ | ä¸­ | æä¾›å¯è§†åŒ–å·¥å…·ï¼Œæ‰‹å·¥å¾®è°ƒ |
| æ™ºèƒ½å¯†é›†åŒ–æ”¶æ•›æ…¢ | ä½ | ä½ | ä¿ç•™å›ºå®šé˜ˆå€¼ä½œä¸ºbackup |
| å¤šå°ºåº¦è®­ç»ƒartifact | ä¸­ | ä¸­ | å¹³æ»‘è¿‡æ¸¡ï¼Œé€æ­¥å¢åŠ åˆ†è¾¨ç‡ |
| é¢œè‰²ç½‘ç»œè¿‡æ‹Ÿåˆ | ä½ | ä½ | æ·»åŠ L2æ­£åˆ™åŒ– |

---

## ç»“è®º

**æ ¸å¿ƒè§‚ç‚¹ï¼š**
æ”¾å¼ƒå½“å‰çš„ä¸‰ä¸ªåˆ›æ–°ç‚¹ï¼ˆVGGæ„ŸçŸ¥æŸå¤±ã€è‡ªé€‚åº”å¯†é›†åŒ–ã€æ—¶åºä¸€è‡´æ€§ï¼‰æ˜¯æ˜æ™ºçš„é€‰æ‹©ã€‚
é€šè¿‡6ä¸ªæ–°çš„è½»é‡çº§åˆ›æ–°ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨**ä»…å¢åŠ 10-15%è®­ç»ƒæ—¶é—´**çš„æƒ…å†µä¸‹ï¼Œ
è¾¾åˆ°**70-90%çš„è´¨é‡æå‡**ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹å¤§å°å’Œæ¨ç†é€Ÿåº¦ã€‚

**æ¨èè¡ŒåŠ¨ï¼š**
1. **ç«‹å³å®æ–½**ï¼šæ–¹æ¡ˆ2ï¼ˆA+B+C+Dï¼‰
2. **å¹¶è¡Œå¼€å‘**ï¼šåˆ›æ–°ç‚¹Eå’ŒFä½œä¸ºbonus
3. **å¿«é€ŸéªŒè¯**ï¼šå…ˆåœ¨å°è§„æ¨¡æ•°æ®é›†ä¸Šæµ‹è¯•ï¼ˆ10k iterationsï¼‰
4. **è¿­ä»£ä¼˜åŒ–**ï¼šæ ¹æ®å®éªŒç»“æœå¾®è°ƒè¶…å‚æ•°

**é¢„æœŸæ—¶é—´çº¿ï¼š**
- Week 1: å®ç°æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼ˆA, Bï¼‰
- Week 2: æ·»åŠ å¢å¼ºåŠŸèƒ½ï¼ˆC, Dï¼‰
- Week 3: å®Œæ•´å®éªŒå’Œè¯„ä¼°
- Week 4: æ–‡æ¡£å’Œå‘å¸ƒ

**æˆåŠŸæ ‡å‡†ï¼š**
- âœ… è®­ç»ƒæ—¶é—´ < 6å°æ—¶ï¼ˆbaselineçš„120%ï¼‰
- âœ… PSNRæå‡ > 0.5 dB
- âœ… é«˜æ–¯ç‚¹æ•° < 150k
- âœ… ä»£ç æ¸…æ™°ï¼Œæ˜“äºç»´æŠ¤

---

**è¿™å¥—æ–°æ–¹æ¡ˆçš„æ•ˆç‡æ¯”æ˜¯å½“å‰æ–¹æ¡ˆçš„20å€ä»¥ä¸Šï¼Œå¼ºçƒˆæ¨èé‡‡ç”¨ï¼**
